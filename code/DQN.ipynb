{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D18LpeysriTQ",
        "outputId": "4a0962ee-a886-4ce4-f0a8-ba9bd5ee85e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/Othercomputers/Il mio laptop/ANN_Project/code\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/Othercomputers/Il mio laptop/ANN_Project/code"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install line_profiler\n",
        "%load_ext line_profiler"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4zz7DII-J4I",
        "outputId": "7cde1025-b2ac-4b78-92d9-6d90b6a25517"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: line_profiler in /usr/local/lib/python3.7/dist-packages (3.5.1)\n",
            "The line_profiler extension is already loaded. To reload it, use:\n",
            "  %reload_ext line_profiler\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OxbcqvQv2h0"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EnL3O2zosmvY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12bebbc0-1c64-4049-f726-7ab021ef08cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%reload_ext autoreload\n",
        "from utils import *\n",
        "from q_learning import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8a99U2q9tXXz"
      },
      "outputs": [],
      "source": [
        "# Configuration paramaters for the whole setup\n",
        "#seed = 0\n",
        "\n",
        "env = TictactoeEnv()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CEanvsZwBRw"
      },
      "source": [
        "### Definition of the Network\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCh80vTDybcs"
      },
      "source": [
        "### Training\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9iCoghIZyfhe"
      },
      "outputs": [],
      "source": [
        "gamma = 0.99  # Discount factor for past rewards\n",
        "epsilon = 1.0  # Epsilon greedy parameter\n",
        "decaying_exploration = True\n",
        "epsilon_min = 0.1  # Minimum epsilon greedy parameter\n",
        "epsilon_max = 0.8  # Maximum epsilon greedy parameter\n",
        "epsilon_interval = (epsilon_max - epsilon_min)  # Rate at which to reduce chance of random action being taken\n",
        "\n",
        "num_episodes = 20000\n",
        "batch_size = 64  # Size of batch taken from replay buffer\n",
        "max_memory_length = 10000 # Size of the replay buffer\n",
        "# How often to update the target network\n",
        "update_target_network = 500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnT8rF1S1V_t",
        "outputId": "9f86c29a-dd5a-4d99-aa06-0aa0f437d581"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_17\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_18 (InputLayer)       [(None, 3, 3, 2)]         0         \n",
            "                                                                 \n",
            " flatten_17 (Flatten)        (None, 18)                0         \n",
            "                                                                 \n",
            " dense_51 (Dense)            (None, 128)               2432      \n",
            "                                                                 \n",
            " dense_52 (Dense)            (None, 128)               16512     \n",
            "                                                                 \n",
            " dense_53 (Dense)            (None, 9)                 1161      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 20,105\n",
            "Trainable params: 20,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Network defined in the project description\n",
        "def create_q_model():\n",
        "    \"\"\"\n",
        "\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    hidden_neurons = 128\n",
        "    num_actions = 9\n",
        "    # Inputs of shape = (3, 3, 2)\n",
        "    inputs = layers.Input(shape=(3, 3, 2,))\n",
        "\n",
        "    layer0 = layers.Flatten()(inputs)\n",
        "    # Two fully connected hidden layers each with 128 neurons and ReLU activation\n",
        "    layer1 = layers.Dense(units=hidden_neurons, activation=\"relu\")(layer0)\n",
        "    layer2 = layers.Dense(units=hidden_neurons, activation=\"relu\")(layer1)\n",
        "    # Output with linear activation function\n",
        "    action = layers.Dense(num_actions, activation=\"linear\")(layer2)\n",
        "\n",
        "    return keras.Model(inputs=inputs, outputs=action)\n",
        "\n",
        "test_model = create_q_model()\n",
        "test_model.summary()\n",
        "\n",
        "\n",
        "def grid_to_tensor(grid, player):\n",
        "    if player == 'X':\n",
        "        return tf.convert_to_tensor(np.stack((np.where(grid == 1, 1, 0), np.where(grid == -1, 1, 0)), -1))\n",
        "    else:\n",
        "        return tf.convert_to_tensor(np.stack((np.where(grid == -1, 1, 0), np.where(grid == 1, 1, 0)), -1))\n",
        "\n",
        "\n",
        "class DeepQPlayer:\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, model, player='X'):\n",
        "        \"\"\"\n",
        "        __init__\n",
        "        :param self: self\n",
        "        :param model:\n",
        "        :param player: 'X' or 'O'\n",
        "        \"\"\"\n",
        "        self.model = model  # initialize model\n",
        "        self.player = player  # set the player\n",
        "\n",
        "    def set_player(self, player='X'):\n",
        "        \"\"\"\n",
        "        Set player to be either 'X' or 'O'\n",
        "        :param self: self\n",
        "        :param player: 'X' or 'O' ('X' by default)\n",
        "        :param j: to change 'X' and 'O'\n",
        "        \"\"\"\n",
        "        self.player = player\n",
        "\n",
        "    def act(self, grid, **kwargs):\n",
        "        \"\"\"\n",
        "        Performs a greedy move, i.e. a (1-epsilon)-greedy action with epsilon equal to zero\n",
        "        :param self: self\n",
        "        :param grid: current state\n",
        "        :param kwargs: keyword arguments\n",
        "        :return: the action chosen greedily\n",
        "        \"\"\"\n",
        "        grid = tf.expand_dims(grid_to_tensor(grid, self.player), axis=0)\n",
        "        action_probs = self.model(grid, training=False)\n",
        "        # Take best action\n",
        "        max_indices = tf.where(action_probs[0] == tf.reduce_max(action_probs[0]))\n",
        "        return int(max_indices[np.random.randint(0, len(max_indices))])  # ties are split randomly\n",
        "\n",
        "\n",
        "def deep_q_learning_against_opt(env, lr=5e-4, gamma=0.99, num_episodes=20000, epsilon_exploration=0.1,\n",
        "                                epsilon_exploration_rule=None, epsilon_opt=0.5, test_freq=None, verbose=False):\n",
        "    \"\"\"\n",
        "\n",
        "    :param lr:\n",
        "    :param env:\n",
        "    :param gamma:\n",
        "    :param num_episodes:\n",
        "    :param epsilon_exploration:\n",
        "    :param epsilon_exploration_rule:\n",
        "    :param epsilon_opt:\n",
        "    :param test_freq:\n",
        "    :param verbose:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    num_actions = 9\n",
        "    batch_size = 64  # Size of batch taken from replay buffer\n",
        "    max_memory_length = 10000  # Size of the replay buffer\n",
        "    # How often to update the target network (num games)\n",
        "    update_target_network = 500\n",
        "    update_freq = 1\n",
        "\n",
        "    # Experience replay buffers\n",
        "    action_history = []\n",
        "    state_history = []\n",
        "    state_next_history = []\n",
        "    rewards_history = []\n",
        "    done_history = []\n",
        "    #episode_reward_history = []\n",
        "    #episode_count = 0\n",
        "    frame_count = 0\n",
        "    #epsilon_random_frames = 0\n",
        "\n",
        "    model = create_q_model()\n",
        "    model_target = create_q_model()\n",
        "    # losing state\n",
        "    losing_state = - np.ones((3, 3))\n",
        "\n",
        "    # Adam optimizer\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
        "    # Using huber loss for stability\n",
        "    loss_function = keras.losses.Huber()\n",
        "\n",
        "    turns = np.array(['X', 'O'])\n",
        "    # Stats of training\n",
        "    episode_rewards = np.empty(num_episodes)\n",
        "    loss_train = np.empty(num_episodes)\n",
        "    if test_freq is not None:\n",
        "        episode_Mopt = [measure_performance(DeepQPlayer(model=model), OptimalPlayer(epsilon=0.))]\n",
        "        episode_Mrand = [measure_performance(DeepQPlayer(model=model), OptimalPlayer(epsilon=1.))]\n",
        "    else:\n",
        "        episode_Mopt = []  # initialize\n",
        "        episode_Mrand = []  # initialize\n",
        "    if verbose and (test_freq is not None):\n",
        "        print('Episode  0 :\\tM_opt = ', episode_Mopt[0], '\\tM_rand = ', episode_Mrand[0])\n",
        "    # Rule for exploration\n",
        "    if epsilon_exploration_rule is None:\n",
        "        def epsilon_exploration_rule(n):\n",
        "            return epsilon_exploration  # if an exploration rule is not given, it is the constant one\n",
        "\n",
        "    for itr in range(num_episodes):\n",
        "        my_player = turns[itr % 2]\n",
        "        player_opt = OptimalPlayer(epsilon=epsilon_opt, player=turns[(itr+1) % 2])\n",
        "        state, _, _ = env.reset()\n",
        "        if env.current_player == player_opt.player:\n",
        "            move = player_opt.act(state)\n",
        "            state, _, _ = env.step(move)\n",
        "        # state = np.array(state)\n",
        "        for i in range(num_actions):\n",
        "            #env.render()\n",
        "            #frame_count += 1\n",
        "\n",
        "            state_tensor = grid_to_tensor(state, my_player)\n",
        "            state_tensor = tf.expand_dims(state_tensor, axis = 0)\n",
        "            # Use epsilon-greedy for exploration\n",
        "            if epsilon_exploration_rule(itr+1) > np.random.uniform(0, 1):\n",
        "                # Take random action\n",
        "                action = np.random.choice(num_actions)\n",
        "            else:\n",
        "                # Predict action Q-values\n",
        "                # From environment state\n",
        "                action_probs = model(state_tensor, training = False)\n",
        "                # Take best action\n",
        "                max_indices = tf.where(action_probs[0] == tf.reduce_max(action_probs[0]))\n",
        "                action = int(max_indices[np.random.randint(0, len(max_indices))])  # ties are split randomly\n",
        "\n",
        "            # Apply the sampled action in our environment\n",
        "            try:\n",
        "              state_adv, _, _ = env.step(action)\n",
        "            except ValueError:\n",
        "              env.end = True\n",
        "              env.winner = player_opt.player\n",
        "            if not env.end:\n",
        "                action_adv = player_opt.act(state_adv)\n",
        "                state_next, _, _ = env.step(action_adv)\n",
        "            else:\n",
        "                ######## DA RIVEDERE #########\n",
        "                state_next = state\n",
        "\n",
        "            reward = env.reward(player=my_player)\n",
        "            done = env.end\n",
        "\n",
        "            frame_count+=1\n",
        "            # Save actions and states in replay buffer\n",
        "            action_history.append(action)\n",
        "            #print(\"------ My player = \", my_player, \"------\")\n",
        "            #print(state)\n",
        "            state_history.append(grid_to_tensor(state, my_player))\n",
        "            #print(state_history[-1][:,:,0])\n",
        "            #print(state_history[-1][:,:,1])\n",
        "            state_next_history.append(grid_to_tensor(state_next, my_player))\n",
        "            #print(state_next_history[-1][:,:,0])\n",
        "            #print(state_next_history[-1][:,:,1])\n",
        "            done_history.append(done)\n",
        "            rewards_history.append(reward)\n",
        "            #if done_history[-1] == True and rewards_history[-1] == 0 :\n",
        "                #print(\"***************Game number:\", itr, \"***************\")\n",
        "                #print(state_history[-1][:,:,0])\n",
        "                #print(state_history[-1][:,:,1])\n",
        "                #print(state_next_history[-1][:,:,0])\n",
        "                #print(state_next_history[-1][:,:,1])\n",
        "            #print(\"Done = \", done_history[-1])\n",
        "            #print(\"Rewards = \", rewards_history[-1])\n",
        "\n",
        "            state = state_next\n",
        "            # Update after every update_freq steps and once batch size is over 64\n",
        "            if frame_count % update_freq == 0 and len(done_history) > batch_size:\n",
        "                # Get indices of samples for replay buffers\n",
        "                indices = np.random.choice(range(len(done_history)), size=batch_size)\n",
        "\n",
        "                # Using list comprehension to sample from replay buffer\n",
        "                state_sample = tf.stack([state_history[i] for i in indices], axis = 0)\n",
        "                state_next_sample =  tf.stack([state_next_history[i] for i in indices], axis = 0)\n",
        "                rewards_sample = [rewards_history[i] for i in indices]\n",
        "                action_sample = [action_history[i] for i in indices]\n",
        "                done_sample = tf.convert_to_tensor([float(done_history[i]) for i in indices])\n",
        "\n",
        "                # Build the updated Q-values for the sampled future states\n",
        "                # Use the target model for stability\n",
        "                start = time.time()\n",
        "                future_rewards = model_target(state_next_sample, training=False) #TODO: COSA CAMBIA CON SENZA\n",
        "                # Q value = reward + discount factor * expected future reward\n",
        "                updated_q_values = rewards_sample + gamma * tf.reduce_max(future_rewards, axis=1) * (1 - done_sample)\n",
        "                \n",
        "                # If final frame set the last value to -1\n",
        "                #updated_q_values = updated_q_values * (1 - done_sample) #- done_sample\n",
        "\n",
        "                # Create a mask so we only calculate loss on the updated Q-values\n",
        "                masks = tf.one_hot(action_sample, num_actions)\n",
        "\n",
        "                with tf.GradientTape() as tape:\n",
        "                    # Train the model on the states and updated Q-values\n",
        "                    q_values = model(state_sample)\n",
        "                    # Apply the masks to the Q-values to get the Q-value for action taken\n",
        "                    q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
        "                    # Calculate loss between new Q-value and old Q-value\n",
        "                    loss = loss_function(updated_q_values, q_action)\n",
        "\n",
        "                # Backpropagation\n",
        "                grads = tape.gradient(loss, model.trainable_variables)\n",
        "                optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "            # Limit the state and reward history\n",
        "            if len(rewards_history) > max_memory_length:\n",
        "                del rewards_history[:1]\n",
        "                del state_history[:1]\n",
        "                del state_next_history[:1]\n",
        "                del action_history[:1]\n",
        "                del done_history[:1]\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        if itr % update_target_network == 0:\n",
        "            # update the target network with new weights\n",
        "            print(\"Updating target network\")\n",
        "            model_target.set_weights(model.get_weights())\n",
        "\n",
        "        episode_rewards[itr] = env.reward(player=my_player)\n",
        "        if len(done_history) > batch_size:\n",
        "            loss_train[itr] = loss\n",
        "        # Testing the performance\n",
        "        if (test_freq is not None) and ((itr+1) % test_freq == 0):\n",
        "            M_opt = measure_performance(DeepQPlayer(model=model), OptimalPlayer(epsilon=0.))\n",
        "            M_rand = measure_performance(DeepQPlayer(model=model), OptimalPlayer(epsilon=1.))\n",
        "            episode_Mopt.append(M_opt)\n",
        "            episode_Mrand.append(M_rand)\n",
        "            #if verbose:\n",
        "            print('Episode ', itr+1, ':\\tM_opt = ', M_opt, '\\tM_rand = ', M_rand)\n",
        "\n",
        "    # Dictionary of stats\n",
        "    stats = {\n",
        "        'rewards': episode_rewards,\n",
        "        'test_Mopt': episode_Mopt,\n",
        "        'test_Mrand': episode_Mrand,\n",
        "        'loss_train': loss_train\n",
        "    }\n",
        "    return model, stats\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "id": "iveZK6ssgH6y",
        "outputId": "7d435d8d-7956-42e9-ba63-8585b7163a99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode  0 :\tM_opt =  -1.0 \tM_rand =  -0.936\n",
            "Updating target network\n",
            "Episode  250 :\tM_opt =  -0.884 \tM_rand =  0.244\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-6829fe29f9b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepsilon_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon_max\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mn_star\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeep_q_learning_against_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mepsilon_exploration_rule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_freq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m250\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-24-a8edefbd1bfd>\u001b[0m in \u001b[0;36mdeep_q_learning_against_opt\u001b[0;34m(env, lr, gamma, num_episodes, epsilon_exploration, epsilon_exploration_rule, epsilon_opt, test_freq, verbose)\u001b[0m\n\u001b[1;32m    224\u001b[0m                     \u001b[0mq_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                     \u001b[0;31m# Calculate loss between new Q-value and old Q-value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdated_q_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m                 \u001b[0;31m# Backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/losses.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[1;32m    141\u001b[0m       \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m       return losses_utils.compute_weighted_loss(\n\u001b[0;32m--> 143\u001b[0;31m           losses, sample_weight, reduction=self._get_reduction())\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/losses_utils.py\u001b[0m in \u001b[0;36mcompute_weighted_loss\u001b[0;34m(losses, sample_weight, reduction, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;31m# Apply reduction function to the individual weighted losses.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce_weighted_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweighted_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m     \u001b[0;31m# Convert the result back to the input type.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/losses_utils.py\u001b[0m in \u001b[0;36mreduce_weighted_loss\u001b[0;34m(weighted_losses, reduction)\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweighted_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreduction\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mReductionV2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSUM_OVER_BATCH_SIZE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_safe_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_num_elements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweighted_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/losses_utils.py\u001b[0m in \u001b[0;36m_num_elements\u001b[0;34m(losses)\u001b[0m\n\u001b[1;32m    255\u001b[0m   \u001b[0;34m\"\"\"Computes the number of elements in `losses` tensor.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'num_elements'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1080\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36msize_v2\u001b[0;34m(input, out_type, name)\u001b[0m\n\u001b[1;32m    742\u001b[0m   \"\"\"\n\u001b[1;32m    743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1080\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36msize\u001b[0;34m(input, name, out_type)\u001b[0m\n\u001b[1;32m    774\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mend_compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m   \"\"\"\n\u001b[0;32m--> 776\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0msize_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    777\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36msize_internal\u001b[0;34m(input, name, optimize, out_type)\u001b[0m\n\u001b[1;32m    797\u001b[0m     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m     \u001b[0mnp_out_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_numpy_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 799\u001b[0;31m     \u001b[0mnum_elements\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp_out_type\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    800\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_elements\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Size\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mprod\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mprod\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   3050\u001b[0m     \"\"\"\n\u001b[1;32m   3051\u001b[0m     return _wrapreduction(a, np.multiply, 'prod', axis, dtype, out,\n\u001b[0;32m-> 3052\u001b[0;31m                           keepdims=keepdims, initial=initial, where=where)\n\u001b[0m\u001b[1;32m   3053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "n_star = 1\n",
        "def rule(n):\n",
        "  return max(epsilon_min, epsilon_max * (1 - n/n_star))\n",
        "\n",
        "model, stats = deep_q_learning_against_opt(env,  epsilon_exploration_rule = rule, verbose = True, test_freq = 250, num_episodes=20000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYDEeSYPr3Uq"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Copy of DQN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}