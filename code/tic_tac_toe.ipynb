{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tic_env import TictactoeEnv, OptimalPlayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tic Toc Toe environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our 1st game is the famous Tic Toc Toe. You can read about the game and its rules here: https://en.wikipedia.org/wiki/Tic-tac-toe\n",
    "\n",
    "We implemented the game as an environment in the style of games in the [Python GYM library](https://gym.openai.com/). The commented source code is available in the file \"tic_env.py\". Here, we give a brief introduction to the environment and how it can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization and attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can initialize the environment / game as following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TictactoeEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which then has the following attributes with the corresponding initial values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'grid': array([[0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.]]),\n 'end': False,\n 'winner': None,\n 'player2value': {'X': 1, 'O': -1},\n 'num_step': 0,\n 'current_player': 'X'}"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game is played by two players: player 'X' and player 'O'. The attribute 'current_player' shows whose turn it is. We assume that player 'X' always plays first.\n",
    "\n",
    "The attribute 'grid' is a 3x3 numpy array and presents the board in the real game and the state $s_t$ in the reinfocement learning language. Each elements can take a value in {0, 1, -1}:\n",
    "     0 : place unmarked\n",
    "     1 : place marked with X \n",
    "    -1 : place marked with O \n",
    "        \n",
    "The attribute 'end' shows if the game is over or not, and the attribute 'winner' shows the winner of the game: either \"X\", \"O\", or None.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use function 'render' to visualize the current position of the board:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|- - -|\n",
      "|- - -|\n",
      "|- - -|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game environment will recieve action from two players in turn and update the grid. At each time, one player can take the action $a_t$, where $a_t$ can either be an integer between 0 to 8 or a touple, corresponding to the 9 possible.\n",
    "\n",
    "Function 'step' is used to recieve the action of the player, update the grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(array([[0., 0., 1.],\n        [0., 0., 0.],\n        [0., 0., 0.]]),\n False,\n None)"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|- - X|\n",
      "|- - -|\n",
      "|- - -|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'grid': array([[0., 0., 1.],\n        [0., 0., 0.],\n        [0., 0., 0.]]),\n 'end': False,\n 'winner': None,\n 'player2value': {'X': 1, 'O': -1},\n 'num_step': 1,\n 'current_player': 'O'}"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(array([[ 0.,  0.,  1.],\n        [ 0., -1.,  0.],\n        [ 0.,  0.,  0.]]),\n False,\n None)"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step((1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|- - X|\n",
      "|- O -|\n",
      "|- - -|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'grid': array([[ 0.,  0.,  1.],\n        [ 0., -1.,  0.],\n        [ 0.,  0.,  0.]]),\n 'end': False,\n 'winner': None,\n 'player2value': {'X': 1, 'O': -1},\n 'num_step': 2,\n 'current_player': 'X'}"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But not all actions are available at each time: One cannot choose a place which has been taken before. There is an error if an unavailable action is taken:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "There is already a chess on position (0, 2).",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_21352\\3516839941.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0menv\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\Documents\\GitHub\\ANN_Project\\code\\tic_env.py\u001B[0m in \u001B[0;36mstep\u001B[1;34m(self, position, print_grid)\u001B[0m\n\u001B[0;32m     68\u001B[0m             \u001B[0mposition\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtuple\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mposition\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     69\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgrid\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mposition\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m!=\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 70\u001B[1;33m             \u001B[1;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'There is already a chess on position {}.'\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mposition\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     71\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     72\u001B[0m         \u001B[1;31m# place a chess on the position\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mValueError\u001B[0m: There is already a chess on position (0, 2)."
     ]
    }
   ],
   "source": [
    "env.step((0,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reward is always 0 until the end of the game. When the game is over, the reward is 1 if you win the game, -1 if you lose, and 0 besides. Function 'observe' can be used after each step to recieve the new state $s_t$, whether the game is over, and the winner, and function 'reward' to get the reward value $r_t$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(array([[ 0.,  0.,  1.],\n        [ 0., -1.,  0.],\n        [ 0.,  0.,  0.]]),\n False,\n None)"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reward(player='X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reward(player='O')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of finishing the game:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(array([[ 1.,  1.,  1.],\n        [-1., -1.,  0.],\n        [ 0.,  0.,  0.]]),\n True,\n 'X')"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(0)\n",
    "env.step(3)\n",
    "env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|X X X|\n",
      "|O O -|\n",
      "|- - -|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(array([[ 1.,  1.,  1.],\n        [-1., -1.,  0.],\n        [ 0.,  0.,  0.]]),\n True,\n 'X')"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reward(player='X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "-1"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reward(player='O')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal policy for Tic Toc Toe environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, we know the exact optimal policy for Tic Toc Toe. We have implemented and $\\epsilon$-greedy version of optimal polciy which you can use for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_player = OptimalPlayer(epsilon = 0., player = 'X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(2, 2)"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_player.act(env.grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'X'"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_player.player"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example of optimal player playing against random player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Game end, winner is player X\n",
      "Optimal player = X\n",
      "Random player = O\n",
      "|- - X|\n",
      "|O X -|\n",
      "|X - O|\n",
      "\n",
      "-------------------------------------------\n",
      "Game end, winner is player O\n",
      "Optimal player = O\n",
      "Random player = X\n",
      "|X - -|\n",
      "|O O O|\n",
      "|X - X|\n",
      "\n",
      "-------------------------------------------\n",
      "Game end, winner is player X\n",
      "Optimal player = X\n",
      "Random player = O\n",
      "|X - O|\n",
      "|- X -|\n",
      "|O - X|\n",
      "\n",
      "-------------------------------------------\n",
      "Game end, winner is player X\n",
      "Optimal player = X\n",
      "Random player = O\n",
      "|X O X|\n",
      "|- O X|\n",
      "|O - X|\n",
      "\n",
      "-------------------------------------------\n",
      "Game end, winner is player O\n",
      "Optimal player = O\n",
      "Random player = X\n",
      "|O - X|\n",
      "|- O X|\n",
      "|X - O|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Turns = np.array(['X','O'])\n",
    "for i in range(5):\n",
    "    env.reset()\n",
    "    grid, _, __ = env.observe()\n",
    "    Turns = Turns[np.random.permutation(2)]\n",
    "    player_opt = OptimalPlayer(epsilon=0., player=Turns[0])\n",
    "    player_rnd = OptimalPlayer(epsilon=1., player=Turns[1])\n",
    "    for j in range(9):\n",
    "        if env.current_player == player_opt.player:\n",
    "            move = player_opt.act(grid)\n",
    "        else:\n",
    "            move = player_rnd.act(grid)\n",
    "\n",
    "        grid, end, winner = env.step(move, print_grid=False)\n",
    "\n",
    "        if end:\n",
    "            print('-------------------------------------------')\n",
    "            print('Game end, winner is player ' + str(winner))\n",
    "            print('Optimal player = ' +  Turns[0])\n",
    "            print('Random player = ' +  Turns[1])\n",
    "            env.render()\n",
    "            env.reset()\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example of optimal player playing against optimal player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Game end, winner is player None\n",
      "Optimal player 1 = X\n",
      "Optimal player 2 = O\n",
      "|O X X|\n",
      "|X X O|\n",
      "|O O X|\n",
      "\n",
      "-------------------------------------------\n",
      "Game end, winner is player None\n",
      "Optimal player 1 = O\n",
      "Optimal player 2 = X\n",
      "|X O X|\n",
      "|O O X|\n",
      "|X X O|\n",
      "\n",
      "-------------------------------------------\n",
      "Game end, winner is player None\n",
      "Optimal player 1 = X\n",
      "Optimal player 2 = O\n",
      "|X O X|\n",
      "|O O X|\n",
      "|X X O|\n",
      "\n",
      "-------------------------------------------\n",
      "Game end, winner is player None\n",
      "Optimal player 1 = X\n",
      "Optimal player 2 = O\n",
      "|O X X|\n",
      "|X O O|\n",
      "|X O X|\n",
      "\n",
      "-------------------------------------------\n",
      "Game end, winner is player None\n",
      "Optimal player 1 = O\n",
      "Optimal player 2 = X\n",
      "|X O X|\n",
      "|X O O|\n",
      "|O X X|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Turns = np.array(['X','O'])\n",
    "for i in range(5):\n",
    "    env.reset()\n",
    "    grid, _, __ = env.observe()\n",
    "    Turns = Turns[np.random.permutation(2)]\n",
    "    player_opt_1 = OptimalPlayer(epsilon=0., player=Turns[0])\n",
    "    player_opt_2 = OptimalPlayer(epsilon=0., player=Turns[1])\n",
    "    for j in range(9):\n",
    "        if env.current_player == player_opt.player:\n",
    "            move = player_opt_1.act(grid)\n",
    "        else:\n",
    "            move = player_opt_2.act(grid)\n",
    "\n",
    "        grid, end, winner = env.step(move, print_grid=False)\n",
    "\n",
    "        if end:\n",
    "            print('-------------------------------------------')\n",
    "            print('Game end, winner is player ' + str(winner))\n",
    "            print('Optimal player 1 = ' +  Turns[0])\n",
    "            print('Optimal player 2 = ' +  Turns[1])\n",
    "            env.render()\n",
    "            env.reset()\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Q-Learning"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def encode_state(state):\n",
    "    return state.tobytes()\n",
    "\n",
    "def available(grid):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    avail_indices = []\n",
    "    avail_mask = [False] * 9\n",
    "    for i in range(9):\n",
    "        pos = (int(i/3), i % 3)\n",
    "        if grid[pos] == 0:\n",
    "            avail_indices.append(i)\n",
    "            avail_mask[i] = True\n",
    "    return avail_indices, avail_mask\n",
    "\n",
    "def epsilon_greedy_action(grid, Q, epsilon, num_actions = 9):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    avail_indices, avail_mask = available(grid)\n",
    "\n",
    "    if np.random.uniform(0,1) < epsilon:\n",
    "        return avail_indices[np.random.randint(0, len(avail_indices))]\n",
    "    else:\n",
    "        q = Q[encode_state(grid)]\n",
    "        q[np.logical_not(avail_mask)] = np.nan\n",
    "        max_indices = np.argwhere(q == np.nanmax(q))\n",
    "        return int(max_indices[np.random.randint(0, len(max_indices))])\n",
    "\n",
    "def q_learning(env, epsilon_exploration = 0.1, epsilon_opt = 0.5, num_episodes = 20000, alpha = 0.5, gamma = 0.99):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    # Q-values map\n",
    "    # Dictionary that maps the np.ndarray.tobyte() representation of the grid to an array of action values\n",
    "    Q = defaultdict(lambda: np.zeros(9))    # All Q-values are initialized to 0\n",
    "    turns = np.array(['X','O'])\n",
    "    episode_rewards = np.zeros(num_episodes)\n",
    "\n",
    "    for itr in range(num_episodes):\n",
    "        my_player = turns[itr % 2]\n",
    "        player_opt = OptimalPlayer(epsilon=epsilon_opt, player=turns[(itr+1) % 2])\n",
    "        env.reset()\n",
    "        state, _, _ = env.observe()\n",
    "        # First step of the adversarial\n",
    "        if env.current_player == player_opt.player:\n",
    "            move = player_opt.act(state)\n",
    "            state, _, _ = env.step(move)\n",
    "        action = epsilon_greedy_action(state, Q, epsilon_exploration)\n",
    "        while not env.end:\n",
    "            next_state, _, _ = env.step(action)     # Move according to the policy\n",
    "            if not env.end:\n",
    "                move = player_opt.act(next_state)   # Adversarial move\n",
    "                next_state, _, _ = env.step(move)\n",
    "            # Sarsa update rule\n",
    "            reward = env.reward(player=my_player)\n",
    "            if not env.end:\n",
    "                next_action = epsilon_greedy_action(next_state, Q, epsilon_exploration)\n",
    "                target = reward + gamma * Q[encode_state(next_state)][next_action]\n",
    "            else:\n",
    "                target = reward\n",
    "            Q[encode_state(state)][action] += alpha * (target - Q[encode_state(state)][action])\n",
    "            # Preparing for the next move\n",
    "            episode_rewards[itr] = reward\n",
    "            state = next_state\n",
    "            action = next_action\n",
    "    return Q, episode_rewards"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def measure_performance(player_1, player_2, num_episodes = 500):\n",
    "    \"\"\"\n",
    "    Measures performance of player 1 against player 2 (percentage of wins player 1)\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    meas = 0\n",
    "    turns = np.array(['X','O'])\n",
    "    env = TictactoeEnv()\n",
    "    for itr in range(num_episodes):\n",
    "        env.reset()\n",
    "        grid, _, __ = env.observe()\n",
    "        player_1.set_player(turns[itr%2])\n",
    "        player_2.set_player(turns[(itr+1)%2])\n",
    "        while not env.end:\n",
    "            if env.current_player == player_1.player:\n",
    "                move = player_1.act(grid)\n",
    "            else:\n",
    "                move = player_2.act(grid)\n",
    "            grid, _, _ = env.step(move, print_grid=False)\n",
    "        meas += env.reward(player=player_1.player)\n",
    "    return meas/num_episodes\n",
    "\n",
    "class QPlayer:\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    def __init__(self, Q, player='X'):\n",
    "        self.Q = Q\n",
    "        self.player = player  # 'x' or 'O'\n",
    "\n",
    "    def set_player(self, player='X', j=-1):\n",
    "        self.player = player\n",
    "        if j != -1:\n",
    "            self.player = 'X' if j % 2 == 0 else 'O'\n",
    "\n",
    "    def act(self, grid, **kwargs):\n",
    "        \"\"\"\n",
    "        TODO\n",
    "        \"\"\"\n",
    "        return epsilon_greedy_action(grid, self.Q, 0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "env = TictactoeEnv()\n",
    "\n",
    "# Hyper-parameters\n",
    "alpha = 0.05    # Learning rate\n",
    "gamma = 0.99    # Discount factor\n",
    "epsilon_opt = 0.5   # Optimal player's epsilon\n",
    "num_episodes = 20000\n",
    "\n",
    "Q, episode_rewards = q_learning(env, epsilon_exploration=0.01, epsilon_opt=epsilon_opt, gamma=gamma, alpha = alpha, num_episodes=num_episodes)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Optimal  player-----\n",
      "M_opt =  0.0\n",
      "M_rand =  0.932\n",
      "\n",
      "----- Random  player-----\n",
      "M_opt =  -0.932\n",
      "M_rand =  -0.054\n",
      "\n",
      "----- Trained  player-----\n",
      "M_opt =  0.0\n",
      "M_rand =  0.87\n"
     ]
    }
   ],
   "source": [
    "turns = np.array(['X','O'])\n",
    "player_opt = OptimalPlayer(epsilon=0.)\n",
    "player_rand = OptimalPlayer(epsilon=1.)\n",
    "\n",
    "names = ['Optimal', 'Random', 'Trained']\n",
    "players = [OptimalPlayer(epsilon=0.), OptimalPlayer(epsilon=1.), QPlayer(Q=Q)]\n",
    "\n",
    "for (name, player) in zip(names, players):\n",
    "    print(\"\\n-----\", name, \" player-----\")\n",
    "    m_opt = measure_performance(player, player_opt)\n",
    "    print(\"M_opt = \", m_opt)\n",
    "    m_rand = measure_performance(player, player_rand)\n",
    "    print(\"M_rand = \", m_rand)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "cs456",
   "language": "python",
   "display_name": "CS456"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}