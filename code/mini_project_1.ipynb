{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tic_env import TictactoeEnv, OptimalPlayer\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cmap\n",
    "import seaborn as sns\n",
    "import scipy.stats as st\n",
    "import time\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Configurations\n",
    "save_stats = False\n",
    "save_figs = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def measure_performance(player_1, player_2, num_episodes = 500):\n",
    "    \"\"\"\n",
    "    Measures performance of player 1 against player 2 (percentage of wins player 1)\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    meas = 0\n",
    "    turns = np.array(['X','O'])\n",
    "    env = TictactoeEnv()\n",
    "    for itr in range(num_episodes):\n",
    "        env.reset()\n",
    "        grid, _, __ = env.observe()\n",
    "        player_1.set_player(turns[itr%2])\n",
    "        player_2.set_player(turns[(itr+1)%2])\n",
    "        while not env.end:\n",
    "            if env.current_player == player_1.player:\n",
    "                move = player_1.act(grid)\n",
    "            else:\n",
    "                move = player_2.act(grid)\n",
    "            grid, _, _ = env.step(move, print_grid=False)\n",
    "        meas += env.reward(player=player_1.player)\n",
    "    return meas/num_episodes\n",
    "\n",
    "def running_average(vec, windows_size = 250):\n",
    "    idx = np.arange(0,len(vec),windows_size)\n",
    "    return [np.sum(vec[i:i+windows_size])/windows_size for i in idx], idx + windows_size\n",
    "\n",
    "def encode_state(state):\n",
    "    return state.tobytes()\n",
    "\n",
    "def available(grid):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    avail_indices = []\n",
    "    avail_mask = [False] * 9\n",
    "    for i in range(9):\n",
    "        pos = (int(i/3), i % 3)\n",
    "        if grid[pos] == 0:\n",
    "            avail_indices.append(i)\n",
    "            avail_mask[i] = True\n",
    "    return avail_indices, avail_mask\n",
    "\n",
    "def epsilon_greedy_action(grid, Q, epsilon, num_actions = 9):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    avail_indices, avail_mask = available(grid)\n",
    "\n",
    "    if np.random.uniform(0,1) < epsilon:\n",
    "        return avail_indices[np.random.randint(0, len(avail_indices))]\n",
    "    else:\n",
    "        q = Q[encode_state(grid)]\n",
    "        q[np.logical_not(avail_mask)] = np.nan\n",
    "        max_indices = np.argwhere(q == np.nanmax(q))\n",
    "        return int(max_indices[np.random.randint(0, len(max_indices))])\n",
    "\n",
    "class QPlayer:\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    def __init__(self, Q, player='X'):\n",
    "        self.Q = Q\n",
    "        self.player = player  # 'x' or 'O'\n",
    "\n",
    "    def set_player(self, player='X', j=-1):\n",
    "        self.player = player\n",
    "        if j != -1:\n",
    "            self.player = 'X' if j % 2 == 0 else 'O'\n",
    "\n",
    "    def act(self, grid, **kwargs):\n",
    "        \"\"\"\n",
    "        TODO\n",
    "        \"\"\"\n",
    "        return epsilon_greedy_action(grid, self.Q, 0)\n",
    "\n",
    "\n",
    "def q_learning(env, alpha = 0.5, gamma = 0.99, num_episodes = 20000, epsilon_exploration = 0.1, epsilon_exploration_rule = None, epsilon_opt = 0.5, test_freq = None, verbose = False):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    turns = np.array(['X','O'])\n",
    "    # Q-values map\n",
    "    # Dictionary that maps the np.ndarray.tobyte() representation of the grid to an array of action values\n",
    "    Q = defaultdict(lambda: np.zeros(9))    # All Q-values are initialized to 0\n",
    "    # Stats of training\n",
    "    episode_rewards = np.zeros(num_episodes)\n",
    "    episode_Mopt = [measure_performance(QPlayer(Q=Q), OptimalPlayer(epsilon=0.))]\n",
    "    episode_Mrand = [measure_performance(QPlayer(Q=Q), OptimalPlayer(epsilon=1.))]\n",
    "    if verbose:\n",
    "                print('Episode  0 :\\tM_opt = ', episode_Mopt[0], '\\tM_rand = ', episode_Mrand[0])\n",
    "    # Rule for exploration\n",
    "    if epsilon_exploration_rule is None:\n",
    "        def epsilon_exploration_rule(n):\n",
    "            return epsilon_exploration\n",
    "\n",
    "    for itr in range(num_episodes):\n",
    "        my_player = turns[itr % 2]\n",
    "        player_opt = OptimalPlayer(epsilon=epsilon_opt, player=turns[(itr+1) % 2])\n",
    "        env.reset()\n",
    "        state, _, _ = env.observe()\n",
    "        # First step of the adversarial\n",
    "        if env.current_player == player_opt.player:\n",
    "            move = player_opt.act(state)\n",
    "            state, _, _ = env.step(move)\n",
    "        action = epsilon_greedy_action(state, Q, epsilon_exploration_rule(itr+1))\n",
    "        while not env.end:\n",
    "            next_state, _, _ = env.step(action)     # Move according to the policy\n",
    "            if not env.end:\n",
    "                move = player_opt.act(next_state)   # Adversarial move\n",
    "                next_state, _, _ = env.step(move)\n",
    "            # Sarsa update rule\n",
    "            reward = env.reward(player=my_player)\n",
    "            if not env.end:\n",
    "                next_action = epsilon_greedy_action(next_state, Q, epsilon_exploration_rule(itr+1))\n",
    "                target = reward + gamma * Q[encode_state(next_state)][next_action]\n",
    "            else:\n",
    "                target = reward\n",
    "            Q[encode_state(state)][action] += alpha * (target - Q[encode_state(state)][action])\n",
    "            # Preparing for the next move\n",
    "            episode_rewards[itr] = reward\n",
    "            state = next_state\n",
    "            action = next_action\n",
    "        # Testing the performance\n",
    "        if (test_freq is not None) and ((itr+1)%test_freq == 0):\n",
    "            M_opt = measure_performance(QPlayer(Q=Q), OptimalPlayer(epsilon=0.))\n",
    "            M_rand = measure_performance(QPlayer(Q=Q), OptimalPlayer(epsilon=1.))\n",
    "            episode_Mopt.append(M_opt)\n",
    "            episode_Mrand.append(M_rand)\n",
    "            if verbose:\n",
    "                print('Episode ', itr+1, ':\\tM_opt = ', M_opt, '\\tM_rand = ', M_rand)\n",
    "    # Dictionary of stats\n",
    "    stats = {\n",
    "        'rewards': episode_rewards,\n",
    "        'test_Mopt': episode_Mopt,\n",
    "        'test_Mrand': episode_Mrand,\n",
    "    }\n",
    "    return Q, stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.1 Learning from experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12940\\4280897795.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mepsilon_exploration\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mQ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mq_learning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon_exploration\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepsilon_exploration\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12940\\939342128.py\u001b[0m in \u001b[0;36mq_learning\u001b[1;34m(env, alpha, gamma, num_episodes, epsilon_exploration, epsilon_exploration_rule, epsilon_opt, test_freq, verbose)\u001b[0m\n\u001b[0;32m    107\u001b[0m             \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m     \u001b[1;31m# Move according to the policy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m                 \u001b[0mmove\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplayer_opt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# Adversarial move\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m                 \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmove\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m             \u001b[1;31m# Sarsa update rule\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Artificial Neural Networks\\ANN_Project\\code\\tic_env.py\u001b[0m in \u001b[0;36mact\u001b[1;34m(self, grid, **kwargs)\u001b[0m\n\u001b[0;32m    359\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mwin\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwin\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    360\u001b[0m         \u001b[1;31m# Block win\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 361\u001b[1;33m         \u001b[0mblock_win\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblockWin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    362\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mblock_win\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mblock_win\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblock_win\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Artificial Neural Networks\\ANN_Project\\code\\tic_env.py\u001b[0m in \u001b[0;36mblockWin\u001b[1;34m(self, grid)\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m         \u001b[0moppon_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplayer\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'X'\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 256\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moppon_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Artificial Neural Networks\\ANN_Project\\code\\tic_env.py\u001b[0m in \u001b[0;36mwin\u001b[1;34m(self, grid, val)\u001b[0m\n\u001b[0;32m    241\u001b[0m             \u001b[0mgrid_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m             \u001b[0mgrid_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 243\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheckWin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrid_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    244\u001b[0m                 \u001b[0mtowin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Artificial Neural Networks\\ANN_Project\\code\\tic_env.py\u001b[0m in \u001b[0;36mcheckWin\u001b[1;34m(self, grid, val)\u001b[0m\n\u001b[0;32m    315\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m         \u001b[1;31m# check diagonals\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 317\u001b[1;33m         \u001b[1;32melif\u001b[0m \u001b[0mgrid\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mgrid\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    318\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = TictactoeEnv()\n",
    "\n",
    "# Hyper-parameters\n",
    "alpha = 0.05    # Learning rate\n",
    "gamma = 0.99    # Discount factor\n",
    "epsilon_opt = 0.5   # Optimal player's epsilon\n",
    "num_episodes = 200\n",
    "epsilon_exploration = 0.01\n",
    "\n",
    "Q, stats = q_learning(env, epsilon_exploration=epsilon_exploration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting the average reward for every 250 games during training\n",
    "running_average_rewards, x = running_average(stats['rewards'])\n",
    "plt.plot(x, running_average_rewards)\n",
    "plt.ylim([-1,1])\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Average reward during training')\n",
    "plt.show()\n",
    "\n",
    "# Comparing the performance with the optimal player and the random player\n",
    "turns = np.array(['X','O'])\n",
    "player_opt = OptimalPlayer(epsilon=0.)\n",
    "player_rand = OptimalPlayer(epsilon=1.)\n",
    "\n",
    "names = ['Optimal', 'Random', 'Trained']\n",
    "players = [OptimalPlayer(epsilon=0.), OptimalPlayer(epsilon=1.), QPlayer(Q=Q)]\n",
    "\n",
    "for (name, player) in zip(names, players):\n",
    "    print(\"\\n-----\", name, \" player-----\")\n",
    "    m_opt = measure_performance(player, player_opt)\n",
    "    print(\"M_opt = \", m_opt)\n",
    "    m_rand = measure_performance(player, player_rand)\n",
    "    print(\"M_rand = \", m_rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 2.1.1 Decreasing exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_freq = 250\n",
    "epsilon_min = 0.1\n",
    "epsilon_max = 0.8\n",
    "vec_n_star = np.hstack((np.array([1, 100, 500, 750]), np.round(np.logspace(3, np.log10(40000), 16))))\n",
    "print(vec_n_star)\n",
    "stats_dict_nstar = {}\n",
    "for n_star in vec_n_star:\n",
    "    print(\"------------- Training with n_star =\", n_star, \"-------------\")\n",
    "    def epsilon_exploration_rule(n):\n",
    "        return np.max([epsilon_min, epsilon_max * (1 - n/n_star)])\n",
    "    start = time.time()\n",
    "    Q, stats = q_learning(env, epsilon_exploration_rule=epsilon_exploration_rule, test_freq=test_freq)\n",
    "    M_opt = measure_performance(QPlayer(Q=Q), OptimalPlayer(epsilon=0.), num_episodes=2000)\n",
    "    M_rand = measure_performance(QPlayer(Q=Q), OptimalPlayer(epsilon=1.), num_episodes=2000)\n",
    "    print(\"M_opt =\", M_opt)\n",
    "    print(\"M_rand =\", M_rand)\n",
    "    stats_dict_nstar.update({n_star: (stats, M_opt, M_rand)})\n",
    "    elapsed = time.time() - start\n",
    "    print(\"Training with n_star =\", n_star, \" took:\", time.strftime(\"%Hh%Mm%Ss\", time.gmtime(elapsed)), \"\\n\\n\")\n",
    "\n",
    "if save_stats:\n",
    "    output_folder = os.path.join(os.getcwd(), 'results')\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    fname = output_folder + '/stats_dict_nstar.pkl'\n",
    "    with open(fname, 'wb') as handle:\n",
    "        pickle.dump(stats_dict_nstar, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_stats(stats_dict, vec_var, var_name, var_legend_name, save = False):\n",
    "    fig_reward, ax_reward = plt.subplots()\n",
    "    fig_performance, ax = plt.subplots(1,2, figsize=(13.4,4.8))\n",
    "\n",
    "    for var in vec_var:\n",
    "        (stats, M_opt, M_rand) = stats_dict[var]\n",
    "        # Plot of the average reward during training\n",
    "        running_average_rewards, x_reward = running_average(stats['rewards'])\n",
    "        ax_reward.plot(x_reward, running_average_rewards, label=\"$\"+ var_legend_name +\" = \"+ str(var) +\"$\")\n",
    "        # Plot of M_opt and M_rand during training\n",
    "        x_performance = np.arange(0, len(stats['rewards'])+1, len(stats['rewards']) / (len(stats['test_Mopt']) - 1))\n",
    "        ax[0].plot(x_performance, stats['test_Mopt'], label=\"$\"+ var_legend_name +\" = \"+ str(var) +\"$\")\n",
    "        ax[1].plot(x_performance, stats['test_Mrand'], label=\"$\"+ var_legend_name +\" = \"+ str(var) +\"$\")\n",
    "        print(var_name + \" =\", var,\": \\tM_opt = \", M_opt, \"\\tM_rand = \", M_rand)\n",
    "\n",
    "    ax_reward.set_ylim([-1,1])\n",
    "    ax_reward.set_xlabel('Episode')\n",
    "    ax_reward.set_ylabel('Reward')\n",
    "    ax_reward.set_title('Average reward during training')\n",
    "    ax_reward.legend(loc='lower right')\n",
    "\n",
    "\n",
    "    ax[0].hlines(y=0, xmin=x[0], xmax=x[-1], color='r', linestyle='--')\n",
    "    ax[0].set_ylim([-1,0.1])\n",
    "    ax[0].set_xlabel('Episode')\n",
    "    ax[0].set_ylabel('$M_{opt}$')\n",
    "    ax[0].set_title('$M_{opt}$ during training')\n",
    "    ax[0].legend(loc='lower right')\n",
    "\n",
    "    ax[1].set_ylim([-1,1])\n",
    "    ax[1].set_xlabel('Episode')\n",
    "    ax[1].set_ylabel('$M_{rand}$')\n",
    "    ax[1].set_title('$M_{rand}$ during training')\n",
    "    ax[1].legend(loc='lower right')\n",
    "    plt.show()\n",
    "    if save:\n",
    "        output_folder = os.path.join(os.getcwd(), 'figures')\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        fig_performance.savefig(output_folder + '/performance_'+var_name+'.png')\n",
    "        fig_reward.savefig(output_folder + '/rewards_'+var_name+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_n_star = vec_n_star\n",
    "plot_stats(stats_dict_nstar, plot_n_star, 'n_star', \"n^{\\star}\", save=save_figs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 2.1.2 Good experts and bad experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_star = 4000 # this should be picked from before not inserted manually\n",
    "#vec_epsilon_opt = [0, 0.3, 0.5, 0.7, 1]\n",
    "M = 10 # how much we want to stratify [0, 1]\n",
    "# a reasonable choice for the epsilon vector here to me is\n",
    "vec_epsilon_opt = np.hstack([np.array([0]), st.uniform.rvs(loc=np.arange(M) / M, scale=1/M), np.array([1])])\n",
    "print(vec_epsilon_opt)\n",
    "stats_dict_epsilon_opt = {}\n",
    "for epsilon_opt in vec_epsilon_opt:\n",
    "    print(\"------------- Training with epsilon_opt =\", epsilon_opt, \"-------------\")\n",
    "    start = time.time()\n",
    "    Q, stats = q_learning(env, epsilon_opt=epsilon_opt, test_freq=test_freq)\n",
    "    M_opt = measure_performance(QPlayer(Q=Q), OptimalPlayer(epsilon=0.), num_episodes=2000)\n",
    "    M_rand = measure_performance(QPlayer(Q=Q), OptimalPlayer(epsilon=1.), num_episodes=2000)\n",
    "    print(\"M_opt =\", M_opt)\n",
    "    print(\"M_rand =\", M_rand)\n",
    "    stats_dict_epsilon_opt.update({epsilon_opt: (stats, M_opt, M_rand)})\n",
    "    elapsed = time.time() - start\n",
    "    print(\"Training with epsilon_opt =\", epsilon_opt, \" took:\", time.strftime(\"%Hh%Mm%Ss\", time.gmtime(elapsed)))\n",
    "\n",
    "if save_stats:\n",
    "    output_folder = os.path.join(os.getcwd(), 'results')\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    fname = output_folder + '/stats_dict_epsilon_opt.pkl'\n",
    "    with open(fname, 'wb') as handle:\n",
    "        pickle.dump(stats_dict_epsilon_opt, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_epsilon_opt = vec_epsilon_opt\n",
    "plot_stats(stats_dict_epsilon_opt, plot_epsilon_opt, \"epsilon_opt\", \"\\epsilon_{opt}\", save=save_figs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.2 Learning by self-practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def q_learning_self_practice(env, alpha = 0.5, gamma = 0.99, num_episodes = 20000,\n",
    "               epsilon_exploration = 0.1, epsilon_exploration_rule = None, epsilon_opt = 0.5,\n",
    "               test_freq = None, verbose = False):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    turns = np.array(['X','O'])\n",
    "    # Q-values map\n",
    "    # Dictionary that maps the np.ndarray.tobyte() representation of the grid to an array of action values\n",
    "    Q = defaultdict(lambda: np.zeros(9))    # All Q-values are initialized to 0\n",
    "    # Stats of training\n",
    "    episode_rewards = np.zeros(num_episodes)\n",
    "    # playing against itself with the same Q-values\n",
    "    episode_Mopt = [measure_performance(QPlayer(Q=Q), OptimalPlayer(epsilon=0.))]\n",
    "    episode_Mrand = [measure_performance(QPlayer(Q=Q), OptimalPlayer(epsilon=1.))]\n",
    "    if verbose:\n",
    "                print('Episode  0 :\\tM_opt = ', episode_Mopt[0], '\\tM_rand = ', episode_Mrand[0])\n",
    "    # Rule for exploration\n",
    "    if epsilon_exploration_rule is None:\n",
    "        def epsilon_exploration_rule(n):\n",
    "            return epsilon_exploration\n",
    "\n",
    "    for itr in range(num_episodes):\n",
    "        my_player = turns[(itr + 1) % 2]\n",
    "        adv_player = turns[itr % 2]\n",
    "        env.reset()\n",
    "        state, _, _ = env.observe()\n",
    "        while not env.end:\n",
    "            if env.current_player == adv_player and not env.end:\n",
    "                action = epsilon_greedy_action(state, Q, epsilon_exploration_rule(itr + 1))\n",
    "                grid, _, _ = env.step(action)\n",
    "                reward =  - env.reward(player=my_player)\n",
    "                if not env.end:\n",
    "                    next_action = epsilon_greedy_action(grid, Q, epsilon_exploration_rule(itr + 1))\n",
    "                    target = reward + gamma * Q[encode_state(grid)][next_action]\n",
    "                else:\n",
    "                    target = reward\n",
    "                Q[encode_state(state)][action] += alpha * (target - Q[encode_state(state)][action])\n",
    "                state = grid\n",
    "            if env.current_player == my_player and not env.end:\n",
    "                action = epsilon_greedy_action(state, Q, epsilon_exploration_rule(itr + 1))\n",
    "                grid, _, _ = env.step(action)\n",
    "                reward = env.reward(player=my_player)\n",
    "                if not env.end:\n",
    "                    next_action = epsilon_greedy_action(grid, Q, epsilon_exploration_rule(itr + 1))\n",
    "                    target = reward + gamma * Q[encode_state(grid)][next_action]\n",
    "                else:\n",
    "                    target = reward\n",
    "                Q[encode_state(state)][action] += alpha * (target - Q[encode_state(state)][action])\n",
    "                state = grid\n",
    "        episode_rewards[itr] = reward\n",
    "        # Testing the performance\n",
    "        if (test_freq is not None) and ((itr+1)%test_freq == 0):\n",
    "            M_opt = measure_performance(QPlayer(Q=Q), OptimalPlayer(epsilon=0.))\n",
    "            M_rand = measure_performance(QPlayer(Q=Q), OptimalPlayer(epsilon=1.))\n",
    "            episode_Mopt.append(M_opt)\n",
    "            episode_Mrand.append(M_rand)\n",
    "            if verbose:\n",
    "                print('Episode ', itr+1, ':\\tM_opt = ', M_opt, '\\tM_rand = ', M_rand)\n",
    "    # Dictionary of stats\n",
    "    \n",
    "    \n",
    "    stats = {\n",
    "        'rewards': episode_rewards,\n",
    "        'test_Mopt': episode_Mopt,\n",
    "        'test_Mrand': episode_Mrand,\n",
    "    }\n",
    "    return Q, stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 7: Fixed $\\epsilon$ first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- Training with epsilon = 0 -------------\n",
      "Episode  0 :\tM_opt =  -0.926 \tM_rand =  0.0\n",
      "Episode  250 :\tM_opt =  -0.92 \tM_rand =  -0.048\n",
      "Episode  500 :\tM_opt =  -0.926 \tM_rand =  0.072\n",
      "Episode  750 :\tM_opt =  -0.936 \tM_rand =  0.07\n",
      "Episode  1000 :\tM_opt =  -0.932 \tM_rand =  0.044\n",
      "------------- Training with epsilon = 0.3 -------------\n",
      "Episode  0 :\tM_opt =  -0.916 \tM_rand =  -0.034\n",
      "Episode  250 :\tM_opt =  -0.926 \tM_rand =  0.006\n",
      "Episode  500 :\tM_opt =  -0.936 \tM_rand =  0.014\n",
      "Episode  750 :\tM_opt =  -0.948 \tM_rand =  0.124\n",
      "Episode  1000 :\tM_opt =  -0.952 \tM_rand =  0.138\n",
      "------------- Training with epsilon = 0.5 -------------\n",
      "Episode  0 :\tM_opt =  -0.916 \tM_rand =  -0.034\n",
      "Episode  250 :\tM_opt =  -0.898 \tM_rand =  -0.05\n",
      "Episode  500 :\tM_opt =  -0.93 \tM_rand =  -0.026\n",
      "Episode  750 :\tM_opt =  -0.934 \tM_rand =  0.068\n",
      "Episode  1000 :\tM_opt =  -0.942 \tM_rand =  0.134\n",
      "------------- Training with epsilon = 0.7 -------------\n",
      "Episode  0 :\tM_opt =  -0.91 \tM_rand =  -0.056\n",
      "Episode  250 :\tM_opt =  -0.918 \tM_rand =  0.034\n",
      "Episode  500 :\tM_opt =  -0.906 \tM_rand =  0.066\n",
      "Episode  750 :\tM_opt =  -0.804 \tM_rand =  0.184\n",
      "Episode  1000 :\tM_opt =  -0.806 \tM_rand =  0.248\n",
      "------------- Training with epsilon = 0.9 -------------\n",
      "Episode  0 :\tM_opt =  -0.934 \tM_rand =  0.03\n",
      "Episode  250 :\tM_opt =  -0.914 \tM_rand =  0.126\n",
      "Episode  500 :\tM_opt =  -0.918 \tM_rand =  0.022\n",
      "Episode  750 :\tM_opt =  -0.918 \tM_rand =  0.088\n",
      "Episode  1000 :\tM_opt =  -0.968 \tM_rand =  0.132\n"
     ]
    }
   ],
   "source": [
    "env = TictactoeEnv()\n",
    "\n",
    "# Hyper-parameters\n",
    "alpha = 0.05    # Learning rate\n",
    "gamma = 0.99    # Discount factor\n",
    "epsilon_opt = 0.5   # Optimal player's epsilon\n",
    "epsilon_vec = [0, 0.3, 0.5, 0.7, 0.9]\n",
    "stats_dic_eps = {}\n",
    "for epsilon in epsilon_vec:\n",
    "    print(\"------------- Training with epsilon =\", epsilon, \"-------------\")\n",
    "    Q, stats = q_learning_self_practice(env, alpha=alpha, gamma=gamma, num_episodes=1000,\n",
    "                        epsilon_exploration=epsilon, test_freq = 250, verbose=True)\n",
    "    stats_dic_eps.update({epsilon: stats})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 8: Changing exploration rule, $\\epsilon$ decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_n_star = 10 * np.arange(1, 10)\n",
    "epsilon_min = 0.1\n",
    "epsilon_max = 0.8\n",
    "for n_star in vec_n_star:\n",
    "    print(\"------------- Training with n_star =\", n_star, \"-------------\")\n",
    "    def epsilon_exploration_rule(n):\n",
    "        return np.max([epsilon_min, epsilon_max * (1 - n/n_star)])\n",
    "    start = time.time()\n",
    "    Q, stats = q_learning_self_practice(env, epsilon_exploration_rule=epsilon_exploration_rule,\n",
    "                                        test_freq=test_freq,  num_episodes = 2000, verbose=True)\n",
    "    M_opt = measure_performance(QPlayer(Q=Q), OptimalPlayer(epsilon=0.), num_episodes=200)\n",
    "    M_rand = measure_performance(QPlayer(Q=Q), OptimalPlayer(epsilon=1.), num_episodes=200)\n",
    "    print(\"M_opt =\", M_opt)\n",
    "    print(\"M_rand =\", M_rand)\n",
    "    stats_dict_nstar.update({n_star: (stats, M_opt, M_rand)})\n",
    "    elapsed = time.time() - start\n",
    "    print(\"Training with n_star =\", n_star, \" took:\", time.strftime(\"%Hh%Mm%Ss\", time.gmtime(elapsed)), \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 9: TODO look for optimal $M_{opt}$ and $M_{rand}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "### look for optimal values of M_opt and M_rand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 10: heat maps in 3 different states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_heat_map(grid, Q):\n",
    "    df = Q[encode_state(grid)][:]\n",
    "    df = np.array([df[i] * (grid[i] != 0) for i in range(len(grid))])\n",
    "    df = np.reshape(df, (3, 3))\n",
    "    heatmap = sns.heatmap(df, annot=True, linewidths=.5)\n",
    "    return heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmaps_subplots(grids, Q):\n",
    "    for i in range(grids.shape[0]):\n",
    "        generate_heat_map(grids[i, :], Q)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Deep Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS456",
   "language": "python",
   "name": "cs456"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
