{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tic_env import TictactoeEnv, OptimalPlayer\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as st\n",
    "import time\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Configurations\n",
    "save_stats = False\n",
    "save_figs = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def measure_performance(player_1, player_2, num_episodes = 500):\n",
    "    \"\"\"\n",
    "    Measures performance of player 1 against player 2 (percentage of wins player 1)\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    meas = 0\n",
    "    turns = np.array(['X','O'])\n",
    "    env = TictactoeEnv()\n",
    "    for itr in range(num_episodes):\n",
    "        env.reset()\n",
    "        grid, _, __ = env.observe()\n",
    "        player_1.set_player(turns[itr%2])\n",
    "        player_2.set_player(turns[(itr+1)%2])\n",
    "        while not env.end:\n",
    "            if env.current_player == player_1.player:\n",
    "                move = player_1.act(grid)\n",
    "            else:\n",
    "                move = player_2.act(grid)\n",
    "            grid, _, _ = env.step(move, print_grid=False)\n",
    "        meas += env.reward(player=player_1.player)\n",
    "    return meas/num_episodes\n",
    "\n",
    "def running_average(vec, windows_size = 250):\n",
    "    idx = np.arange(0,len(vec),windows_size)\n",
    "    return [np.sum(vec[i:i+windows_size])/windows_size for i in idx], idx + windows_size\n",
    "\n",
    "def encode_state(state):\n",
    "    return state.tobytes()\n",
    "\n",
    "def available(grid):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    avail_indices = []\n",
    "    avail_mask = [False] * 9\n",
    "    for i in range(9):\n",
    "        pos = (int(i/3), i % 3)\n",
    "        if grid[pos] == 0:\n",
    "            avail_indices.append(i)\n",
    "            avail_mask[i] = True\n",
    "    return avail_indices, avail_mask\n",
    "\n",
    "def epsilon_greedy_action(grid, Q, epsilon, num_actions = 9):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    avail_indices, avail_mask = available(grid)\n",
    "\n",
    "    if np.random.uniform(0,1) < epsilon:\n",
    "        return avail_indices[np.random.randint(0, len(avail_indices))]\n",
    "    else:\n",
    "        q = Q[encode_state(grid)]\n",
    "        q[np.logical_not(avail_mask)] = np.nan\n",
    "        max_indices = np.argwhere(q == np.nanmax(q))\n",
    "        return int(max_indices[np.random.randint(0, len(max_indices))])\n",
    "\n",
    "class QPlayer:\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    def __init__(self, Q, player='X'):\n",
    "        self.Q = Q\n",
    "        self.player = player  # 'x' or 'O'\n",
    "\n",
    "    def set_player(self, player='X', j=-1):\n",
    "        self.player = player\n",
    "        if j != -1:\n",
    "            self.player = 'X' if j % 2 == 0 else 'O'\n",
    "\n",
    "    def act(self, grid, **kwargs):\n",
    "        \"\"\"\n",
    "        TODO\n",
    "        \"\"\"\n",
    "        return epsilon_greedy_action(grid, self.Q, 0)\n",
    "\n",
    "\n",
    "def q_learning(env, alpha = 0.5, gamma = 0.99, num_episodes = 20000, epsilon_exploration = 0.1, epsilon_exploration_rule = None, epsilon_opt = 0.5, test_freq = None, verbose = False):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    turns = np.array(['X','O'])\n",
    "    # Q-values map\n",
    "    # Dictionary that maps the np.ndarray.tobyte() representation of the grid to an array of action values\n",
    "    Q = defaultdict(lambda: np.zeros(9))    # All Q-values are initialized to 0\n",
    "    # Stats of training\n",
    "    episode_rewards = np.zeros(num_episodes)\n",
    "    episode_Mopt = [measure_performance(QPlayer(Q=Q), OptimalPlayer(epsilon=0.))]\n",
    "    episode_Mrand = [measure_performance(QPlayer(Q=Q), OptimalPlayer(epsilon=0.))]\n",
    "    if verbose:\n",
    "                print('Episode  0 :\\tM_opt = ', episode_Mopt[0], '\\tM_rand = ', episode_Mrand[0])\n",
    "    # Rule for exploration\n",
    "    if epsilon_exploration_rule is None:\n",
    "        def epsilon_exploration_rule(n):\n",
    "            return epsilon_exploration\n",
    "\n",
    "    for itr in range(num_episodes):\n",
    "        my_player = turns[itr % 2]\n",
    "        player_opt = OptimalPlayer(epsilon=epsilon_opt, player=turns[(itr+1) % 2])\n",
    "        env.reset()\n",
    "        state, _, _ = env.observe()\n",
    "        # First step of the adversarial\n",
    "        if env.current_player == player_opt.player:\n",
    "            move = player_opt.act(state)\n",
    "            state, _, _ = env.step(move)\n",
    "        action = epsilon_greedy_action(state, Q, epsilon_exploration_rule(itr+1))\n",
    "        while not env.end:\n",
    "            next_state, _, _ = env.step(action)     # Move according to the policy\n",
    "            if not env.end:\n",
    "                move = player_opt.act(next_state)   # Adversarial move\n",
    "                next_state, _, _ = env.step(move)\n",
    "            # Sarsa update rule\n",
    "            reward = env.reward(player=my_player)\n",
    "            if not env.end:\n",
    "                next_action = epsilon_greedy_action(next_state, Q, epsilon_exploration_rule(itr+1))\n",
    "                target = reward + gamma * Q[encode_state(next_state)][next_action]\n",
    "            else:\n",
    "                target = reward\n",
    "            Q[encode_state(state)][action] += alpha * (target - Q[encode_state(state)][action])\n",
    "            # Preparing for the next move\n",
    "            episode_rewards[itr] = reward\n",
    "            state = next_state\n",
    "            action = next_action\n",
    "        # Testing the performance\n",
    "        if (test_freq is not None) and ((itr+1)%test_freq == 0):\n",
    "            M_opt = measure_performance(QPlayer(Q=Q), OptimalPlayer(epsilon=0.))\n",
    "            M_rand = measure_performance(QPlayer(Q=Q), OptimalPlayer(epsilon=1.))\n",
    "            episode_Mopt.append(M_opt)\n",
    "            episode_Mrand.append(M_rand)\n",
    "            if verbose:\n",
    "                print('Episode ', itr+1, ':\\tM_opt = ', M_opt, '\\tM_rand = ', M_rand)\n",
    "    # Dictionary of stats\n",
    "    stats = {\n",
    "        'rewards': episode_rewards,\n",
    "        'test_Mopt': episode_Mopt,\n",
    "        'test_Mrand': episode_Mrand,\n",
    "    }\n",
    "    return Q, stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.1 Learning from experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19876\\2122930047.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mepsilon_exploration\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mQ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mq_learning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon_exploration\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepsilon_exploration\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19876\\1532814387.py\u001b[0m in \u001b[0;36mq_learning\u001b[1;34m(env, alpha, gamma, num_episodes, epsilon_exploration, epsilon_exploration_rule, epsilon_opt, test_freq, verbose)\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepsilon_greedy_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon_exploration_rule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitr\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m             \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m     \u001b[1;31m# Move according to the policy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m                 \u001b[0mmove\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplayer_opt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# Adversarial move\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Artificial Neural Networks\\ANN_Project\\code\\tic_env.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, position, print_grid)\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_player\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'X'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_step\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m'O'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[1;31m# check whether the game ends or not\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheckEnd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mprint_grid\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Artificial Neural Networks\\ANN_Project\\code\\tic_env.py\u001b[0m in \u001b[0;36mcheckEnd\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     95\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwinner\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'O'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[1;31m# check diagonals\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwinner\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'X'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = TictactoeEnv()\n",
    "\n",
    "# Hyper-parameters\n",
    "alpha = 0.05    # Learning rate\n",
    "gamma = 0.99    # Discount factor\n",
    "epsilon_opt = 0.5   # Optimal player's epsilon\n",
    "num_episodes = 20000\n",
    "epsilon_exploration = 0.01\n",
    "\n",
    "Q, stats = q_learning(env, epsilon_exploration=epsilon_exploration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting the average reward for every 250 games during training\n",
    "running_average_rewards, x = running_average(stats['rewards'])\n",
    "plt.plot(x, running_average_rewards)\n",
    "plt.ylim([-1,1])\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Average reward during training')\n",
    "plt.show()\n",
    "\n",
    "# Comparing the performance with the optimal player and the random player\n",
    "turns = np.array(['X','O'])\n",
    "player_opt = OptimalPlayer(epsilon=0.)\n",
    "player_rand = OptimalPlayer(epsilon=1.)\n",
    "\n",
    "names = ['Optimal', 'Random', 'Trained']\n",
    "players = [OptimalPlayer(epsilon=0.), OptimalPlayer(epsilon=1.), QPlayer(Q=Q)]\n",
    "\n",
    "for (name, player) in zip(names, players):\n",
    "    print(\"\\n-----\", name, \" player-----\")\n",
    "    m_opt = measure_performance(player, player_opt)\n",
    "    print(\"M_opt = \", m_opt)\n",
    "    m_rand = measure_performance(player, player_rand)\n",
    "    print(\"M_rand = \", m_rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 2.1.1 Decreasing exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_freq = 250\n",
    "epsilon_min = 0.1\n",
    "epsilon_max = 0.8\n",
    "vec_n_star = np.hstack((np.array([1, 100, 500, 750]), np.round(np.logspace(3, np.log10(40000), 16))))\n",
    "print(vec_n_star)\n",
    "stats_dict_nstar = {}\n",
    "for n_star in vec_n_star:\n",
    "    print(\"------------- Training with n_star =\", n_star, \"-------------\")\n",
    "    def epsilon_exploration_rule(n):\n",
    "        return np.max([epsilon_min, epsilon_max * (1 - n/n_star)])\n",
    "    start = time.time()\n",
    "    Q, stats = q_learning(env, epsilon_exploration_rule=epsilon_exploration_rule, test_freq=test_freq)\n",
    "    M_opt = measure_performance(QPlayer(Q=Q), OptimalPlayer(epsilon=0.), num_episodes=2000)\n",
    "    M_rand = measure_performance(QPlayer(Q=Q), OptimalPlayer(epsilon=1.), num_episodes=2000)\n",
    "    print(\"M_opt =\", M_opt)\n",
    "    print(\"M_rand =\", M_rand)\n",
    "    stats_dict_nstar.update({n_star: (stats, M_opt, M_rand)})\n",
    "    elapsed = time.time() - start\n",
    "    print(\"Training with n_star =\", n_star, \" took:\", time.strftime(\"%Hh%Mm%Ss\", time.gmtime(elapsed)), \"\\n\\n\")\n",
    "\n",
    "if save_stats:\n",
    "    output_folder = os.path.join(os.getcwd(), 'results')\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    fname = output_folder + '/stats_dict_nstar.pkl'\n",
    "    with open(fname, 'wb') as handle:\n",
    "        pickle.dump(stats_dict_nstar, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_stats(stats_dict, vec_var, var_name, var_legend_name, save = False):\n",
    "    fig_reward, ax_reward = plt.subplots()\n",
    "    fig_performance, ax = plt.subplots(1,2, figsize=(13.4,4.8))\n",
    "\n",
    "    for var in vec_var:\n",
    "        (stats, M_opt, M_rand) = stats_dict[var]\n",
    "        # Plot of the average reward during training\n",
    "        running_average_rewards, x_reward = running_average(stats['rewards'])\n",
    "        ax_reward.plot(x_reward, running_average_rewards, label=\"$\"+ var_legend_name +\" = \"+ str(var) +\"$\")\n",
    "        # Plot of M_opt and M_rand during training\n",
    "        x_performance = np.arange(0, len(stats['rewards'])+1, len(stats['rewards']) / (len(stats['test_Mopt']) - 1))\n",
    "        ax[0].plot(x_performance, stats['test_Mopt'], label=\"$\"+ var_legend_name +\" = \"+ str(var) +\"$\")\n",
    "        ax[1].plot(x_performance, stats['test_Mrand'], label=\"$\"+ var_legend_name +\" = \"+ str(var) +\"$\")\n",
    "        print(var_name + \" =\", var,\": \\tM_opt = \", M_opt, \"\\tM_rand = \", M_rand)\n",
    "\n",
    "    ax_reward.set_ylim([-1,1])\n",
    "    ax_reward.set_xlabel('Episode')\n",
    "    ax_reward.set_ylabel('Reward')\n",
    "    ax_reward.set_title('Average reward during training')\n",
    "    ax_reward.legend(loc='lower right')\n",
    "\n",
    "\n",
    "    ax[0].hlines(y=0, xmin=x[0], xmax=x[-1], color='r', linestyle='--')\n",
    "    ax[0].set_ylim([-1,0.1])\n",
    "    ax[0].set_xlabel('Episode')\n",
    "    ax[0].set_ylabel('$M_{opt}$')\n",
    "    ax[0].set_title('$M_{opt}$ during training')\n",
    "    ax[0].legend(loc='lower right')\n",
    "\n",
    "    ax[1].set_ylim([-1,1])\n",
    "    ax[1].set_xlabel('Episode')\n",
    "    ax[1].set_ylabel('$M_{rand}$')\n",
    "    ax[1].set_title('$M_{rand}$ during training')\n",
    "    ax[1].legend(loc='lower right')\n",
    "    plt.show()\n",
    "    if save:\n",
    "        output_folder = os.path.join(os.getcwd(), 'figures')\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        fig_performance.savefig(output_folder + '/performance_'+var_name+'.png')\n",
    "        fig_reward.savefig(output_folder + '/rewards_'+var_name+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_n_star = vec_n_star\n",
    "plot_stats(stats_dict_nstar, plot_n_star, 'n_star', \"n^{\\star}\", save=save_figs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 2.1.2 Good experts and bad experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_star = 4000 # this should be picked from before not inserted manually\n",
    "#vec_epsilon_opt = [0, 0.3, 0.5, 0.7, 1]\n",
    "M = 10 # how much we want to stratify [0, 1]\n",
    "# a reasonable choice for the epsilon vector here to me is\n",
    "vec_epsilon_opt = np.hstack([np.array([0]), st.uniform.rvs(loc=np.arange(M) / M, scale=1/M), np.array([1])])\n",
    "print(vec_epsilon_opt)\n",
    "stats_dict_epsilon_opt = {}\n",
    "for epsilon_opt in vec_epsilon_opt:\n",
    "    print(\"------------- Training with epsilon_opt =\", epsilon_opt, \"-------------\")\n",
    "    start = time.time()\n",
    "    Q, stats = q_learning(env, epsilon_opt=epsilon_opt, test_freq=test_freq)\n",
    "    M_opt = measure_performance(QPlayer(Q=Q), OptimalPlayer(epsilon=0.), num_episodes=2000)\n",
    "    M_rand = measure_performance(QPlayer(Q=Q), OptimalPlayer(epsilon=1.), num_episodes=2000)\n",
    "    print(\"M_opt =\", M_opt)\n",
    "    print(\"M_rand =\", M_rand)\n",
    "    stats_dict_epsilon_opt.update({epsilon_opt: (stats, M_opt, M_rand)})\n",
    "    elapsed = time.time() - start\n",
    "    print(\"Training with epsilon_opt =\", epsilon_opt, \" took:\", time.strftime(\"%Hh%Mm%Ss\", time.gmtime(elapsed)))\n",
    "\n",
    "if save_stats:\n",
    "    output_folder = os.path.join(os.getcwd(), 'results')\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    fname = output_folder + '/stats_dict_epsilon_opt.pkl'\n",
    "    with open(fname, 'wb') as handle:\n",
    "        pickle.dump(stats_dict_epsilon_opt, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_epsilon_opt = vec_epsilon_opt\n",
    "plot_stats(stats_dict_epsilon_opt, plot_epsilon_opt, \"epsilon_opt\", \"\\epsilon_{opt}\", save=save_figs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.2 Learning by self-practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def q_learning_self_practice(env, alpha = 0.5, gamma = 0.99, num_episodes = 20000,\n",
    "               epsilon_exploration = 0.1, epsilon_exploration_rule = None, epsilon_opt = 0.5,\n",
    "               test_freq = None, verbose = False):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    turns = np.array(['X','O'])\n",
    "    # Q-values map\n",
    "    # Dictionary that maps the np.ndarray.tobyte() representation of the grid to an array of action values\n",
    "    Q = defaultdict(lambda: np.zeros(9))    # All Q-values are initialized to 0\n",
    "    # Stats of training\n",
    "    episode_rewards = np.zeros(num_episodes)\n",
    "    # playing against itself with the same Q-values\n",
    "    episode_Mopt = [measure_performance(QPlayer(Q=Q), OptimalPlayer(epsilon=0.))]\n",
    "    episode_Mrand = [measure_performance(QPlayer(Q=Q), OptimalPlayer(epsilon=1.))]\n",
    "    if verbose:\n",
    "                print('Episode  0 :\\tM_opt = ', episode_Mopt[0], '\\tM_rand = ', episode_Mrand[0])\n",
    "    # Rule for exploration\n",
    "    if epsilon_exploration_rule is None:\n",
    "        def epsilon_exploration_rule(n):\n",
    "            return epsilon_exploration\n",
    "\n",
    "    for itr in range(num_episodes):\n",
    "        my_player = turns[(itr + 1) % 2]\n",
    "        adv_player = turns[itr % 2]\n",
    "        env.reset()\n",
    "        state, _, _ = env.observe()\n",
    "        while not env.end:\n",
    "            if env.current_player == adv_player and not env.end:\n",
    "                action_adv = epsilon_greedy_action(state, Q, epsilon_exploration_rule(itr + 1))\n",
    "                grid, _, _ = env.step(action_adv)\n",
    "                adv_reward = env.reward(player=adv_player)\n",
    "                my_reward = - 1 * adv_reward\n",
    "                next_state_adv, _, _ = env.observe()\n",
    "                if not env.end:\n",
    "                    next_action_adv = epsilon_greedy_action(grid, Q, epsilon_exploration_rule(itr + 1))\n",
    "                    target = adv_reward + gamma * Q[encode_state(next_state_adv)][next_action_adv]\n",
    "                else:\n",
    "                    target = adv_reward\n",
    "                Q[encode_state(state)][action_adv] += alpha * (target - Q[encode_state(state)][action_adv])\n",
    "                state = next_state_adv\n",
    "            if env.current_player == my_player and not env.end:\n",
    "                my_action = epsilon_greedy_action(state, Q, epsilon_exploration_rule(itr + 1))\n",
    "                grid, _, _ = env.step(my_action)\n",
    "                my_reward = env.reward(player=my_player)\n",
    "                my_next_state, _, _ = env.observe()\n",
    "                if not env.end:\n",
    "                    my_next_action = epsilon_greedy_action(grid, Q, epsilon_exploration_rule(itr + 1))\n",
    "                    target = my_reward + gamma * Q[encode_state(my_next_state)][my_next_action]\n",
    "                else:\n",
    "                    target = my_reward\n",
    "                Q[encode_state(state)][my_action] += alpha * (target - Q[encode_state(state)][my_action]) \n",
    "                # Preparing for the next move\n",
    "            episode_rewards[itr] = my_reward\n",
    "            state = my_next_state\n",
    "            action = my_next_action\n",
    "        # Testing the performance\n",
    "        if (test_freq is not None) and ((itr+1)%test_freq == 0):\n",
    "            M_opt = measure_performance(QPlayer(Q=Q), OptimalPlayer(epsilon=0.))\n",
    "            M_rand = measure_performance(QPlayer(Q=Q), OptimalPlayer(epsilon=1.))\n",
    "            episode_Mopt.append(M_opt)\n",
    "            episode_Mrand.append(M_rand)\n",
    "            if verbose:\n",
    "                print('Episode ', itr+1, ':\\tM_opt = ', M_opt, '\\tM_rand = ', M_rand)\n",
    "    # Dictionary of stats\n",
    "    stats = {\n",
    "        'rewards': episode_rewards,\n",
    "        'test_Mopt': episode_Mopt,\n",
    "        'test_Mrand': episode_Mrand,\n",
    "    }\n",
    "    return Q, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- Training with epsilon = 0 -------------\n",
      "Episode  0 :\tM_opt =  -0.912 \tM_rand =  0.028\n",
      "Episode  1000 :\tM_opt =  -0.922 \tM_rand =  0.092\n",
      "Episode  2000 :\tM_opt =  -0.922 \tM_rand =  0.048\n",
      "Episode  3000 :\tM_opt =  -0.934 \tM_rand =  0.07\n",
      "Episode  4000 :\tM_opt =  -0.914 \tM_rand =  0.116\n",
      "Episode  5000 :\tM_opt =  -0.924 \tM_rand =  0.1\n",
      "Episode  6000 :\tM_opt =  -0.932 \tM_rand =  0.092\n",
      "Episode  7000 :\tM_opt =  -0.92 \tM_rand =  0.02\n",
      "Episode  8000 :\tM_opt =  -0.932 \tM_rand =  0.018\n",
      "Episode  9000 :\tM_opt =  -0.938 \tM_rand =  0.07\n",
      "Episode  10000 :\tM_opt =  -0.93 \tM_rand =  0.076\n",
      "------------- Training with epsilon = 0.3 -------------\n",
      "Episode  0 :\tM_opt =  -0.93 \tM_rand =  0.032\n",
      "Episode  1000 :\tM_opt =  -0.916 \tM_rand =  0.182\n",
      "Episode  2000 :\tM_opt =  -0.982 \tM_rand =  0.244\n",
      "Episode  3000 :\tM_opt =  -0.994 \tM_rand =  0.32\n",
      "Episode  4000 :\tM_opt =  -0.978 \tM_rand =  0.368\n",
      "Episode  5000 :\tM_opt =  -0.998 \tM_rand =  0.428\n",
      "Episode  6000 :\tM_opt =  -0.998 \tM_rand =  0.384\n",
      "Episode  7000 :\tM_opt =  -0.996 \tM_rand =  0.378\n",
      "Episode  8000 :\tM_opt =  -1.0 \tM_rand =  0.38\n",
      "Episode  9000 :\tM_opt =  -0.996 \tM_rand =  0.456\n",
      "Episode  10000 :\tM_opt =  -0.996 \tM_rand =  0.452\n",
      "------------- Training with epsilon = 0.5 -------------\n",
      "Episode  0 :\tM_opt =  -0.934 \tM_rand =  0.014\n",
      "Episode  1000 :\tM_opt =  -0.96 \tM_rand =  0.182\n",
      "Episode  2000 :\tM_opt =  -0.984 \tM_rand =  0.306\n",
      "Episode  3000 :\tM_opt =  -0.992 \tM_rand =  0.314\n",
      "Episode  4000 :\tM_opt =  -0.994 \tM_rand =  0.326\n",
      "Episode  5000 :\tM_opt =  -0.992 \tM_rand =  0.33\n",
      "Episode  6000 :\tM_opt =  -0.998 \tM_rand =  0.396\n",
      "Episode  7000 :\tM_opt =  -0.994 \tM_rand =  0.36\n",
      "Episode  8000 :\tM_opt =  -0.99 \tM_rand =  0.436\n",
      "Episode  9000 :\tM_opt =  -0.998 \tM_rand =  0.384\n",
      "Episode  10000 :\tM_opt =  -0.988 \tM_rand =  0.362\n",
      "------------- Training with epsilon = 0.7 -------------\n",
      "Episode  0 :\tM_opt =  -0.946 \tM_rand =  0.024\n",
      "Episode  1000 :\tM_opt =  -0.964 \tM_rand =  0.268\n",
      "Episode  2000 :\tM_opt =  -0.99 \tM_rand =  0.35\n",
      "Episode  3000 :\tM_opt =  -0.988 \tM_rand =  0.368\n",
      "Episode  4000 :\tM_opt =  -0.984 \tM_rand =  0.374\n",
      "Episode  5000 :\tM_opt =  -0.976 \tM_rand =  0.46\n",
      "Episode  6000 :\tM_opt =  -0.976 \tM_rand =  0.408\n",
      "Episode  7000 :\tM_opt =  -0.976 \tM_rand =  0.504\n",
      "Episode  8000 :\tM_opt =  -1.0 \tM_rand =  0.394\n",
      "Episode  9000 :\tM_opt =  -1.0 \tM_rand =  0.476\n",
      "Episode  10000 :\tM_opt =  -1.0 \tM_rand =  0.474\n",
      "------------- Training with epsilon = 1 -------------\n",
      "Episode  0 :\tM_opt =  -0.932 \tM_rand =  0.016\n",
      "Episode  1000 :\tM_opt =  -0.938 \tM_rand =  0.08\n",
      "Episode  2000 :\tM_opt =  -0.906 \tM_rand =  0.194\n",
      "Episode  3000 :\tM_opt =  -0.966 \tM_rand =  0.132\n",
      "Episode  4000 :\tM_opt =  -0.964 \tM_rand =  0.246\n",
      "Episode  5000 :\tM_opt =  -0.958 \tM_rand =  0.302\n",
      "Episode  6000 :\tM_opt =  -1.0 \tM_rand =  0.35\n",
      "Episode  7000 :\tM_opt =  -0.888 \tM_rand =  0.34\n",
      "Episode  8000 :\tM_opt =  -1.0 \tM_rand =  0.498\n",
      "Episode  9000 :\tM_opt =  -0.934 \tM_rand =  0.412\n",
      "Episode  10000 :\tM_opt =  -0.928 \tM_rand =  0.422\n"
     ]
    }
   ],
   "source": [
    "env = TictactoeEnv()\n",
    "\n",
    "# Hyper-parameters\n",
    "alpha = 0.05    # Learning rate\n",
    "gamma = 0.99    # Discount factor\n",
    "epsilon_opt = 0.5   # Optimal player's epsilon\n",
    "epsilon_vec = [0, 0.3, 0.5, 0.7, 1]\n",
    "stats_dic_eps = {}\n",
    "for epsilon in epsilon_vec:\n",
    "    print(\"------------- Training with epsilon =\", epsilon, \"-------------\")\n",
    "    Q, stats = q_learning_self_practice(env, alpha=alpha, gamma=gamma, num_episodes=10000,\n",
    "                        epsilon_exploration=epsilon, test_freq = 1000, verbose=True)\n",
    "    stats_dic_eps.update({epsilon: stats})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Deep Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS456",
   "language": "python",
   "name": "cs456"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
