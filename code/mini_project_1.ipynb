{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tic_env import TictactoeEnv, OptimalPlayer\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cmap\n",
    "import seaborn as sns\n",
    "import scipy.stats as st\n",
    "import time\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Configurations\n",
    "save_stats = False\n",
    "save_figs = True\n",
    "load = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Utils for Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_action(grid, Q, epsilon):\n",
    "    \"\"\"\n",
    "    Performs a (1-epsilon)-greedy action starting from the given state and the given Q-values\n",
    "    @param grid: current state\n",
    "    @param Q: current Q-values\n",
    "    @param epsilon: exploration parameter\n",
    "    \"\"\"\n",
    "    # get the available positions\n",
    "    avail_indices, avail_mask = available(grid)\n",
    "\n",
    "    if np.random.uniform(0,1) < epsilon:\n",
    "         # with probability epsilon make a random move (exploration)\n",
    "        return avail_indices[np.random.randint(0, len(avail_indices))]\n",
    "    else:\n",
    "        # with probability 1-epsilon choose the action with highest immediate reward (exploitation)\n",
    "        q = Q[encode_state(grid)]\n",
    "        q[np.logical_not(avail_mask)] = np.nan # set all the Q(state, action) with action currently non available to nan \n",
    "        max_indices = np.argwhere(q == np.nanmax(q)) # best action(s) along the available ones\n",
    "        return int(max_indices[np.random.randint(0, len(max_indices))]) # ties are split randomly\n",
    "\n",
    "class QPlayer:\n",
    "    \"\"\"\n",
    "    Class for an agent which plays according to the current updates of the Q-values during the learning\n",
    "    \"\"\"\n",
    "    def __init__(self, Q, player='X'):\n",
    "        \"\"\"\n",
    "        __init__\n",
    "        @param self: self\n",
    "        @param Q: current Q-values\n",
    "        @param player: 'X' or 'O'\n",
    "        \"\"\"\n",
    "        self.Q = Q # initialize Q-values\n",
    "        self.player = player\n",
    "\n",
    "    def set_player(self, player='X', j=-1):\n",
    "        \"\"\"\n",
    "        Set player to be either 'X' or 'O'\n",
    "        @param self: self\n",
    "        @param player: 'X' or 'O' ('X' by default)\n",
    "        @param j: to change 'X' and 'O'\n",
    "        \"\"\"\n",
    "        self.player = player\n",
    "        if j != -1:\n",
    "            self.player = 'X' if j % 2 == 0 else 'O'\n",
    "\n",
    "    def act(self, grid, **kwargs):\n",
    "        \"\"\"\n",
    "        Performs a greedy move, i.e. a (1-epsilon)-greedy action with epsilon equal to zero\n",
    "        @param self: self\n",
    "        @param grid: current state\n",
    "        @param kwargs: keyword arguments\n",
    "        @return: the action chosen greedily\n",
    "        \"\"\"\n",
    "        return epsilon_greedy_action(grid, self.Q, 0)\n",
    "    \n",
    "def measure_performance(player_1, player_2, num_episodes = 500):\n",
    "    \"\"\"\n",
    "    Measures performance of player 1 against player 2\n",
    "    @param player_1: first player (usually a QPlayer member)\n",
    "    @param player_2: second player (usually a OptimalPlayer member)\n",
    "    @param num_episodes: number of episodes played\n",
    "    @return: percentage of wins of player_1 against player_2\n",
    "    \"\"\"\n",
    "    meas = 0\n",
    "    turns = np.array(['X','O'])\n",
    "    env = TictactoeEnv() # setting the environment\n",
    "    for itr in range(num_episodes):\n",
    "        env.reset() # reset at the beginning of each episode\n",
    "        grid, _, _ = env.observe()\n",
    "        # alternating the turns to start\n",
    "        player_1.set_player(turns[itr%2])\n",
    "        player_2.set_player(turns[(itr+1)%2])\n",
    "        while not env.end:\n",
    "            if env.current_player == player_1.player:\n",
    "                move = player_1.act(grid) # move of the first player\n",
    "            else:\n",
    "                move = player_2.act(grid) # move of the second player\n",
    "            grid, _, _ = env.step(move, print_grid=False) # updating the environment\n",
    "        meas += env.reward(player=player_1.player) # updating the reward of player_1\n",
    "    return meas/num_episodes\n",
    "\n",
    "def running_average(vec, windows_size = 250):\n",
    "    \"\"\"\n",
    "    Computes the running average of vec every windows_size elements\n",
    "    Example: if windows_size=250 then it computes the mean of vec from 1 to 250, from 251 to 500 and so on...\n",
    "    @param vec: numpy.ndarray\n",
    "    @param windows_size: windows_size\n",
    "    @return: mean of vec every windows_size elements\n",
    "    \"\"\"\n",
    "    idx = np.arange(0, len(vec), windows_size) # (i * windows_size for i = 1, ..., len(vec) / windows_size))\n",
    "    return [np.sum(vec[i:i+windows_size])/windows_size for i in idx], idx + windows_size\n",
    "\n",
    "def encode_state(state):\n",
    "    \"\"\"\n",
    "    Constructs bytes containing the raw data bytes in the array.\n",
    "    @param state: numpy.ndarray\n",
    "    @return: the bytes representation of the state\n",
    "    \"\"\"\n",
    "    return state.tobytes()\n",
    "\n",
    "def available(grid):\n",
    "    \"\"\"\n",
    "    Getter for the available positions given a grid\n",
    "    @param grid: current state\n",
    "    @return: available indices and available mask (numpy.ndarray True or False)\n",
    "    \"\"\"\n",
    "    avail_indices = []\n",
    "    avail_mask = [False] * 9\n",
    "    for i in range(9):\n",
    "        pos = (int(i/3), i % 3)\n",
    "        if grid[pos] == 0:\n",
    "            avail_indices.append(i) # add i to the available indices\n",
    "            avail_mask[i] = True # set the mask of the position i to True\n",
    "    return avail_indices, avail_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning_against_opt(env, alpha = 0.5, gamma = 0.99, num_episodes = 20000, epsilon_exploration = 0.1,\n",
    "               epsilon_exploration_rule = None, epsilon_opt = 0.5, test_freq = None, verbose = False):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    turns = np.array(['X','O'])\n",
    "    # Q-values map\n",
    "    # Dictionary that maps the np.ndarray.tobyte() representation of the grid to an array of action values\n",
    "    Q = defaultdict(lambda: np.zeros(9))    # All Q-values are initialized to 0\n",
    "    # Stats of training\n",
    "    episode_rewards = np.empty(num_episodes)\n",
    "    if test_freq is not None:\n",
    "        episode_Mopt = [measure_performance(QPlayer(Q=Q), OptimalPlayer(epsilon=0.))]\n",
    "        episode_Mrand = [measure_performance(QPlayer(Q=Q), OptimalPlayer(epsilon=1.))]\n",
    "    else:\n",
    "        episode_Mopt = []\n",
    "        episode_Mrand = []\n",
    "    if verbose and (test_freq is not None):\n",
    "                print('Episode  0 :\\tM_opt = ', episode_Mopt[0], '\\tM_rand = ', episode_Mrand[0])\n",
    "    # Rule for exploration\n",
    "    if epsilon_exploration_rule is None:\n",
    "        def epsilon_exploration_rule(n):\n",
    "            return epsilon_exploration\n",
    "\n",
    "    for itr in range(num_episodes):\n",
    "        my_player = turns[itr % 2]\n",
    "        player_opt = OptimalPlayer(epsilon=epsilon_opt, player=turns[(itr+1) % 2])\n",
    "        env.reset()\n",
    "        state, _, _ = env.observe()\n",
    "        # First step of the adversarial\n",
    "        if env.current_player == player_opt.player:\n",
    "            move = player_opt.act(state)\n",
    "            state, _, _ = env.step(move)\n",
    "        action = epsilon_greedy_action(state, Q, epsilon_exploration_rule(itr+1))\n",
    "        while not env.end:\n",
    "            next_state, _, _ = env.step(action)     # Move according to the policy\n",
    "            if not env.end:\n",
    "                move = player_opt.act(next_state)   # Adversarial move\n",
    "                next_state, _, _ = env.step(move)\n",
    "            # Sarsa update rule\n",
    "            reward = env.reward(player=my_player)\n",
    "            if not env.end:\n",
    "                next_action = epsilon_greedy_action(next_state, Q, epsilon_exploration_rule(itr+1))\n",
    "                target = reward + gamma * Q[encode_state(next_state)][next_action]\n",
    "            else:\n",
    "                target = reward\n",
    "            Q[encode_state(state)][action] += alpha * (target - Q[encode_state(state)][action])\n",
    "            # Preparing for the next move\n",
    "            state = next_state\n",
    "            action = next_action\n",
    "\n",
    "        episode_rewards[itr] = env.reward(player=my_player)\n",
    "\n",
    "        # Testing the performance\n",
    "        if (test_freq is not None) and ((itr+1)%test_freq == 0):\n",
    "            M_opt = measure_performance(QPlayer(Q=Q), OptimalPlayer(epsilon=0.))\n",
    "            M_rand = measure_performance(QPlayer(Q=Q), OptimalPlayer(epsilon=1.))\n",
    "            episode_Mopt.append(M_opt)\n",
    "            episode_Mrand.append(M_rand)\n",
    "            if verbose:\n",
    "                print('Episode ', itr+1, ':\\tM_opt = ', M_opt, '\\tM_rand = ', M_rand)\n",
    "    # Dictionary of stats\n",
    "    stats = {\n",
    "        'rewards': episode_rewards,\n",
    "        'test_Mopt': episode_Mopt,\n",
    "        'test_Mrand': episode_Mrand,\n",
    "    }\n",
    "    return Q, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning_self_practice(env, alpha = 0.5, gamma = 0.99, num_episodes = 20000, epsilon_exploration = 0.1,\n",
    "                             epsilon_exploration_rule = None, test_freq = None, verbose = False):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    turns = np.array(['X','O'])\n",
    "    # Q-values map\n",
    "    # Dictionary that maps the np.ndarray.tobyte() representation of the grid to an array of action values\n",
    "    Q = defaultdict(lambda: np.zeros(9))    # All Q-values are initialized to 0\n",
    "    # Stats of training\n",
    "    episode_rewards = np.empty(num_episodes)\n",
    "    # Stats of training\n",
    "    if test_freq is not None:\n",
    "        episode_Mopt = [measure_performance(QPlayer(Q=Q), OptimalPlayer(epsilon=0.))]\n",
    "        episode_Mrand = [measure_performance(QPlayer(Q=Q), OptimalPlayer(epsilon=1.))]\n",
    "    else:\n",
    "        episode_Mopt = []\n",
    "        episode_Mrand = []\n",
    "    if verbose and (test_freq is not None):\n",
    "                print('Episode  0 :\\tM_opt = ', episode_Mopt[0], '\\tM_rand = ', episode_Mrand[0])\n",
    "    # Rule for exploration\n",
    "    if epsilon_exploration_rule is None:\n",
    "        def epsilon_exploration_rule(n):\n",
    "            return epsilon_exploration\n",
    "\n",
    "    for itr in range(num_episodes):\n",
    "        my_player = turns[itr % 2]\n",
    "        env.reset()\n",
    "        state, _, _ = env.observe()\n",
    "        # First two turns outside the loop (at least five turns are played)\n",
    "        action = epsilon_greedy_action(state, Q, epsilon_exploration_rule(itr + 1))\n",
    "        state_adv, _, _ = env.step(action)\n",
    "        action_adv = epsilon_greedy_action(state_adv, Q, epsilon_exploration_rule(itr + 1))\n",
    "        while not env.end:\n",
    "            # Adversarial turn\n",
    "            state_adv, _, _ = env.observe()\n",
    "            reward = - env.reward(player=env.current_player)    # Reward of the player who made the last move\n",
    "            next_state, _, _ = env.step(action_adv)\n",
    "            # Player's turn\n",
    "            if not env.end:\n",
    "                next_action = epsilon_greedy_action(next_state, Q, epsilon_exploration_rule(itr + 1))\n",
    "                target = reward + gamma * Q[encode_state(next_state)][next_action]\n",
    "            else:   # action_adv is the one that makes the game end\n",
    "                reward = - env.reward(player=env.current_player)    # reward of the player who made the game end, i.e. the adversary of the current player\n",
    "                # Update for the adversary of the current player\n",
    "                Q[encode_state(state_adv)][action_adv] += alpha * reward\n",
    "                # Target for the current player\n",
    "                target = - reward + gamma * Q[encode_state(next_state)][next_action]\n",
    "            Q[encode_state(state)][action] += alpha * (target - Q[encode_state(state)][action])\n",
    "\n",
    "            # Preparing for the next iteration\n",
    "            action = action_adv\n",
    "            state = state_adv\n",
    "            action_adv = next_action\n",
    "\n",
    "\n",
    "        episode_rewards[itr] = env.reward(player=my_player)\n",
    "        # Testing the performance\n",
    "        if (test_freq is not None) and ((itr+1)%test_freq == 0):\n",
    "            M_opt = measure_performance(QPlayer(Q=Q), OptimalPlayer(epsilon=0.))\n",
    "            M_rand = measure_performance(QPlayer(Q=Q), OptimalPlayer(epsilon=1.))\n",
    "            episode_Mopt.append(M_opt)\n",
    "            episode_Mrand.append(M_rand)\n",
    "            if verbose:\n",
    "                print('Episode ', itr+1, ':\\tM_opt = ', M_opt, '\\tM_rand = ', M_rand)\n",
    "    \n",
    "    # Dictionary of stats\n",
    "    stats = {\n",
    "        'rewards': episode_rewards,\n",
    "        'test_Mopt': episode_Mopt,\n",
    "        'test_Mrand': episode_Mrand,\n",
    "    }\n",
    "    return Q, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stats(stats_dict, vec_var, var_name, var_legend_name, save = False):\n",
    "    \"\"\"\n",
    "    Creates the plots for reward and M_opt, M_rand\n",
    "    @param stats_dict: dictionary containing all the stats \n",
    "    @param vec_var: vector of all the variables\n",
    "    @param var_name: variables name\n",
    "    @param var_legend_name: variables name for the legend\n",
    "    @param save: true to save figures in output folder\n",
    "    \"\"\"\n",
    "    # creating the environment for the two plots\n",
    "    fig_reward, ax_reward = plt.subplots()\n",
    "    fig_performance, ax = plt.subplots(1,2, figsize=(13.4,4.8))\n",
    "    fig_performance.subplots_adjust(top=0.9, left=0.1, right=0.9, bottom=0.12) # adjust the spacing between subplots\n",
    "\n",
    "    for var in vec_var:\n",
    "        (stats, M_opt, M_rand) = stats_dict[var]\n",
    "        # Plot of the average reward during training\n",
    "        running_average_rewards, x_reward = running_average(stats['rewards']) # compute average reward\n",
    "        ax_reward.plot(x_reward, running_average_rewards, label=\"$\"+ var_legend_name +\" = \"+ str(var) +\"$\")\n",
    "        # Plot of M_opt and M_rand during training\n",
    "        x_performance = np.arange(0, len(stats['rewards'])+1, len(stats['rewards']) / (len(stats['test_Mopt']) - 1))\n",
    "        ax[0].plot(x_performance, stats['test_Mopt'], label=\"$\"+ var_legend_name +\" = \"+ str(var) +\"$\")\n",
    "        ax[1].plot(x_performance, stats['test_Mrand'], label=\"$\"+ var_legend_name +\" = \"+ str(var) +\"$\")\n",
    "        print(var_name + \" =\", var,\": \\tM_opt = \", M_opt, \"\\tM_rand = \", M_rand) # print the performance\n",
    "\n",
    "    ax_reward.set_ylim([-1,1])\n",
    "    ax_reward.set_xlabel('Episode')\n",
    "    ax_reward.set_ylabel('Reward')\n",
    "    ax_reward.set_title('Average reward during training')\n",
    "    ax_reward.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), \n",
    "                     fancybox=True, shadow=True, ncol=5, fontsize=10)  # legend below outside the plot\n",
    "\n",
    "\n",
    "    ax[0].hlines(y=0, xmin=x_reward[0], xmax=x_reward[-1],\n",
    "                 color='r', linestyle='--') # plot also the zero line for M_opt, as it is the highest M_opt achieved\n",
    "    ax[0].set_ylim([-1,0.1])\n",
    "    ax[0].set_xlabel('Episode')\n",
    "    ax[0].set_ylabel('$M_{opt}$')\n",
    "    ax[0].set_title('$M_{opt}$ during training')\n",
    "\n",
    "    ax[1].set_ylim([-1,1])\n",
    "    ax[1].set_xlabel('Episode')\n",
    "    ax[1].set_ylabel('$M_{rand}$')\n",
    "    ax[1].set_title('$M_{rand}$ during training')\n",
    "    \n",
    "    ax.flatten()[-2].legend(loc='upper center', bbox_to_anchor=(1.1, -0.15),\n",
    "                            fancybox=True, shadow=True, ncol=5, fontsize=10)  # unique legend for the two plots\n",
    "    plt.show()\n",
    "    \n",
    "    # saving onto file\n",
    "    if save:\n",
    "        output_folder = os.path.join(os.getcwd(), 'figures') # set the output folder\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        # saving figures in \"eps\" format\n",
    "        fig_performance.savefig(output_folder + '/performance_'+var_name+'.png')\n",
    "        fig_reward.savefig(output_folder + '/rewards_'+var_name+'.png')\n",
    "        fig_performance.savefig(output_folder + '/performance_'+var_name+'.eps', format='eps')\n",
    "        fig_reward.savefig(output_folder + '/rewards_'+var_name+'.eps', format='eps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, alpha = 0.5, gamma = 0.99, num_episodes = 20000, epsilon_exploration = 0.1,\n",
    "                             epsilon_exploration_rule = None, epsilon_opt = 0.5, test_freq = None, verbose = False, \n",
    "                             against_opt=False, self_practice=False):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    if (int(against_opt) + int(self_practice) != 1):\n",
    "        raise ValueError(\"Please choose a training method\")\n",
    "    if against_opt:\n",
    "        return q_learning_against_opt(env, alpha, gamma, num_episodes, epsilon_exploration,\n",
    "                             epsilon_exploration_rule,  epsilon_opt, test_freq, verbose)\n",
    "    else:\n",
    "        return q_learning_self_practice(env, alpha, gamma, num_episodes, epsilon_exploration,\n",
    "                             epsilon_exploration_rule, test_freq, verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env = TictactoeEnv()\n",
    "\n",
    "# Hyper-parameters\n",
    "alpha = 0.05    # Learning rate\n",
    "gamma = 0.99    # Discount factor\n",
    "epsilon_opt = 0.5   # Optimal player's epsilon\n",
    "num_episodes = 20000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.1 Learning from experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "epsilon_vec_plot = np.array([0, 0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1])\n",
    "epsilon_vec = np.hstack((epsilon_vec_plot, np.setdiff1d(np.logspace(-4, -1, 10), epsilon_vec_plot)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if not load:\n",
    "    env = TictactoeEnv()\n",
    "    stats_dict_eps = {}\n",
    "    for epsilon in epsilon_vec:\n",
    "        print(\"------------- Training with epsilon =\", epsilon, \"-------------\")\n",
    "        start = time.time()\n",
    "        Q, stats = q_learning(env, num_episodes=num_episodes,\n",
    "                              epsilon_exploration=epsilon, test_freq=250, verbose=False, against_opt=True)\n",
    "        M_opt = measure_performance(QPlayer(Q=Q), OptimalPlayer(epsilon=0.), num_episodes=2000)\n",
    "        M_rand = measure_performance(QPlayer(Q=Q), OptimalPlayer(epsilon=1.), num_episodes=2000)\n",
    "        print(\"M_opt =\", M_opt)\n",
    "        print(\"M_rand =\", M_rand)\n",
    "        stats_dict_eps.update({epsilon: (stats, M_opt, M_rand)})\n",
    "        elapsed = time.time() - start\n",
    "        print(\"Training with epsilon =\", epsilon, \" took:\", time.strftime(\"%Hh%Mm%Ss\", time.gmtime(elapsed)), \"\\n\\n\")\n",
    "\n",
    "    if save_stats:\n",
    "        output_folder = os.path.join(os.getcwd(), 'results')\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        fname = output_folder + '/stats_dict_eps.pkl'\n",
    "        with open(fname, 'wb') as handle:\n",
    "            pickle.dump(stats_dict_eps, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if load:\n",
    "    output_folder = os.path.join(os.getcwd(), 'results')\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    fname = output_folder + '/stats_dict_eps_self.pkl'\n",
    "    with open(fname, 'rb') as handle:\n",
    "        stats_dict_eps = pickle.load(handle)\n",
    "\n",
    "plot_stats(stats_dict_eps, epsilon_vec_plot, \"epsilon\", \"\\epsilon\", save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "epsilon_exploration = 0.05\n",
    "Q, stats = q_learning(env, epsilon_exploration=epsilon_exploration, num_episodes=num_episodes, verbose=False, against_opt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Trained  player-----\n",
      "M_opt =  0.0\n",
      "M_rand =  0.788\n",
      "\n",
      "----- Optimal  player-----\n",
      "M_opt =  0.0\n",
      "M_rand =  0.92\n",
      "\n",
      "----- Random  player-----\n",
      "M_opt =  -0.932\n",
      "M_rand =  0.0\n"
     ]
    }
   ],
   "source": [
    "# Plotting the average reward for every 250 games during training\n",
    "running_average_rewards, x = running_average(stats['rewards'])\n",
    "fig = plt.figure()\n",
    "plt.plot(x, running_average_rewards)\n",
    "plt.ylim([-1,1])\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Average reward during training')\n",
    "plt.show()\n",
    "if save_figs:\n",
    "        output_folder = os.path.join(os.getcwd(), 'figures')\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        fig.savefig(output_folder + '/rewards_Q1.png')\n",
    "        fig.savefig(output_folder + '/rewards_Q1.eps', format = 'eps')\n",
    "\n",
    "# Comparing the performance with the optimal player and the random player\n",
    "turns = np.array(['X','O'])\n",
    "player_opt = OptimalPlayer(epsilon=0.)\n",
    "player_rand = OptimalPlayer(epsilon=1.)\n",
    "\n",
    "names = ['Trained', 'Optimal', 'Random']\n",
    "players = [QPlayer(Q=Q), OptimalPlayer(epsilon=0.), OptimalPlayer(epsilon=1.)]\n",
    "\n",
    "for (name, player) in zip(names, players):\n",
    "    print(\"\\n-----\", name, \" player-----\")\n",
    "    m_opt = measure_performance(player, player_opt)\n",
    "    print(\"M_opt = \", m_opt)\n",
    "    m_rand = measure_performance(player, player_rand)\n",
    "    print(\"M_rand = \", m_rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 2.1.1 Decreasing exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_freq = 250\n",
    "epsilon_min = 0.1\n",
    "epsilon_max = 0.8\n",
    "vec_n_star = np.hstack((np.array([1, 100, 500, 750]), np.round(np.logspace(3, np.log10(40000), 16))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if not load:\n",
    "    stats_dict_nstar = {}\n",
    "    for n_star in vec_n_star:\n",
    "        print(\"------------- Training with n_star =\", n_star, \"-------------\")\n",
    "        def epsilon_exploration_rule(n):\n",
    "            return np.max([epsilon_min, epsilon_max * (1 - n/n_star)])\n",
    "        start = time.time()\n",
    "        Q, stats = q_learning(env, epsilon_exploration_rule=epsilon_exploration_rule, test_freq=test_freq, against_opt=True)\n",
    "        M_opt = measure_performance(QPlayer(Q=Q), OptimalPlayer(epsilon=0.), num_episodes=2000)\n",
    "        M_rand = measure_performance(QPlayer(Q=Q), OptimalPlayer(epsilon=1.), num_episodes=2000)\n",
    "        print(\"M_opt =\", M_opt)\n",
    "        print(\"M_rand =\", M_rand)\n",
    "        stats_dict_nstar.update({n_star: (stats, M_opt, M_rand)})\n",
    "        elapsed = time.time() - start\n",
    "        print(\"Training with n_star =\", n_star, \" took:\", time.strftime(\"%Hh%Mm%Ss\", time.gmtime(elapsed)), \"\\n\\n\")\n",
    "\n",
    "    if save_stats:\n",
    "        output_folder = os.path.join(os.getcwd(), 'results')\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        fname = output_folder + '/stats_dict_nstar.pkl'\n",
    "        with open(fname, 'wb') as handle:\n",
    "            pickle.dump(stats_dict_nstar, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if load:\n",
    "    output_folder = os.path.join(os.getcwd(), 'results')\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    fname = output_folder + '/stats_dict_nstar.pkl'\n",
    "    with open(fname, 'rb') as handle:\n",
    "        stats_dict_nstar = pickle.load(handle)\n",
    "\n",
    "plot_n_star = [1., 24460., 40000]\n",
    "plot_stats(stats_dict_nstar, plot_n_star, 'n_star', \"n^{\\star}\", save=True)\n",
    "\n",
    "plot_n_star = [1., 4373., 9146.]\n",
    "plot_stats(stats_dict_nstar, plot_n_star, 'n_star', \"n^{\\star}\", save=True)\n",
    "# plot_stats(stats_dict_nstar, vec_n_star, 'n_star', \"n^{\\star}\", save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 2.1.2 Good experts and bad experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_star = 4000 # this should be picked from before not inserted manually\n",
    "M = 11\n",
    "vec_epsilon_opt = np.linspace(0,1,M)\n",
    "print(vec_epsilon_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if not load:\n",
    "    stats_dict_epsilon_opt = {}\n",
    "    for epsilon_opt in vec_epsilon_opt:\n",
    "        print(\"------------- Training with epsilon_opt =\", epsilon_opt, \"-------------\")\n",
    "        start = time.time()\n",
    "        Q, stats = q_learning(env, epsilon_opt=epsilon_opt, test_freq=test_freq, against_opt=True)\n",
    "        M_opt = measure_performance(QPlayer(Q=Q), OptimalPlayer(epsilon=0.), num_episodes=2000)\n",
    "        M_rand = measure_performance(QPlayer(Q=Q), OptimalPlayer(epsilon=1.), num_episodes=2000)\n",
    "        print(\"M_opt =\", M_opt)\n",
    "        print(\"M_rand =\", M_rand)\n",
    "        stats_dict_epsilon_opt.update({epsilon_opt: (stats, M_opt, M_rand)})\n",
    "        elapsed = time.time() - start\n",
    "        print(\"Training with epsilon_opt =\", epsilon_opt, \" took:\", time.strftime(\"%Hh%Mm%Ss\", time.gmtime(elapsed)))\n",
    "\n",
    "    if save_stats:\n",
    "        output_folder = os.path.join(os.getcwd(), 'results')\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        fname = output_folder + '/stats_dict_epsilon_opt.pkl'\n",
    "        with open(fname, 'wb') as handle:\n",
    "            pickle.dump(stats_dict_epsilon_opt, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if load:\n",
    "    output_folder = os.path.join(os.getcwd(), 'results')\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    fname = output_folder + '/stats_dict_epsilon_opt.pkl'\n",
    "    with open(fname, 'rb') as handle:\n",
    "        stats_dict_epsilon_opt = pickle.load(handle)\n",
    "\n",
    "plot_epsilon_opt = [0., 0.5, 1.]\n",
    "plot_stats(stats_dict_epsilon_opt, plot_epsilon_opt, \"epsilon_opt\", \"\\epsilon_{opt}\", save=True)\n",
    "# plot_stats(stats_dict_epsilon_opt, vec_epsilon_opt, \"epsilon_opt\", \"\\epsilon_{opt}\", save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.2 Learning by self-practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 7: Fixed $\\epsilon$ first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "epsilon_vec_plot = np.array([0, 0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1])\n",
    "epsilon_vec = np.hstack((epsilon_vec_plot, np.setdiff1d(np.logspace(-4, -1, 10), epsilon_vec_plot)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if not load:\n",
    "    env = TictactoeEnv()\n",
    "    stats_dict_eps_self = {}\n",
    "    for epsilon in epsilon_vec:\n",
    "        print(\"------------- Training with epsilon =\", epsilon, \"-------------\")\n",
    "        Q, stats = q_learning(env, num_episodes=num_episodes,\n",
    "                            epsilon_exploration=epsilon, test_freq = 250, verbose=False, self_practice=True)\n",
    "        M_opt = measure_performance(QPlayer(Q=Q), OptimalPlayer(epsilon=0.), num_episodes=2000)\n",
    "        M_rand = measure_performance(QPlayer(Q=Q), OptimalPlayer(epsilon=1.), num_episodes=2000)\n",
    "        print(\"M_opt =\", M_opt)\n",
    "        print(\"M_rand =\", M_rand)\n",
    "        stats_dict_eps_self.update({epsilon: (stats, M_opt, M_rand)})\n",
    "\n",
    "    if save_stats:\n",
    "        output_folder = os.path.join(os.getcwd(), 'results')\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        fname = output_folder + '/stats_dict_eps_self.pkl'\n",
    "        with open(fname, 'wb') as handle:\n",
    "            pickle.dump(stats_dict_eps_self, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if load:\n",
    "    output_folder = os.path.join(os.getcwd(), 'results')\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    fname = output_folder + '/stats_dict_eps_self.pkl'\n",
    "    with open(fname, 'rb') as handle:\n",
    "        stats_dict_eps_self = pickle.load(handle)\n",
    "\n",
    "epsilon_vec_plot = [0, 0.005, 0.1, 0.5]\n",
    "plot_stats(stats_dict_eps_self, epsilon_vec_plot, \"epsilon_self\", \"\\epsilon\", save=save_figs)\n",
    "plot_stats(stats_dict_eps_self, epsilon_vec, \"epsilon_self\", \"\\epsilon\", save=save_figs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 8: Changing exploration rule, $\\epsilon$ decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "vec_n_star = np.hstack((np.array([1, 100, 500, 750]), np.round(np.logspace(3, np.log10(40000), 16))))\n",
    "epsilon_min = 0.1\n",
    "epsilon_max = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not load:\n",
    "    stats_dict_nstar_self = {}\n",
    "    for n_star in vec_n_star:\n",
    "        print(\"------------- Training with n_star =\", n_star, \"-------------\")\n",
    "        def epsilon_exploration_rule(n):\n",
    "            return np.max([epsilon_min, epsilon_max * (1 - n/n_star)])\n",
    "        start = time.time()\n",
    "        Q, stats = q_learning(env, epsilon_exploration_rule=epsilon_exploration_rule,\n",
    "                                test_freq=test_freq,  num_episodes=num_episodes, verbose=False, self_practice=True)\n",
    "        M_opt = measure_performance(QPlayer(Q=Q), OptimalPlayer(epsilon=0.), num_episodes=200)\n",
    "        M_rand = measure_performance(QPlayer(Q=Q), OptimalPlayer(epsilon=1.), num_episodes=200)\n",
    "        print(\"M_opt =\", M_opt)\n",
    "        print(\"M_rand =\", M_rand)\n",
    "        stats_dict_nstar_self.update({n_star: (stats, M_opt, M_rand)})\n",
    "        elapsed = time.time() - start\n",
    "        print(\"Training with n_star =\", n_star, \"took:\", time.strftime(\"%Hh%Mm%Ss\", time.gmtime(elapsed)), \"\\n\\n\")\n",
    "\n",
    "    if save_stats:\n",
    "        output_folder = os.path.join(os.getcwd(), 'results')\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        fname = output_folder + '/stats_dict_nstar_self.pkl'\n",
    "        with open(fname, 'wb') as handle:\n",
    "            pickle.dump(stats_dict_nstar_self, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if load:\n",
    "    output_folder = os.path.join(os.getcwd(), 'results')\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    fname = output_folder + '/stats_dict_nstar_self.pkl'\n",
    "    with open(fname, 'rb') as handle:\n",
    "        stats_dict_nstar_self = pickle.load(handle)\n",
    "\n",
    "plot_n_star = [1., 4373., 24460., 40000]\n",
    "plot_stats(stats_dict_nstar_self, plot_n_star, 'n_star', \"n^{\\star}\", save=save_figs)\n",
    "\n",
    "plot_n_star = [1., 4373., 9146.]\n",
    "plot_stats(stats_dict_nstar_self, plot_n_star, 'n_star', \"n^{\\star}\", save=save_figs)\n",
    "plot_stats(stats_dict_nstar_self, vec_n_star, 'n_star', \"n^{\\star}\", save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 9: TODO look for optimal $M_{opt}$ and $M_{rand}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### look for optimal values of M_opt and M_rand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 10: heat maps in 3 different states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Parametri ottimali di Q\n",
    "epsilon_min = 0.1\n",
    "epsilon_max = 0.8\n",
    "n_star = 4373\n",
    "def epsilon_exploration_rule(n):\n",
    "            return np.max([epsilon_min, epsilon_max * (1 - n/n_star)])\n",
    "Q, stats = q_learning(env, epsilon_exploration_rule=epsilon_exploration_rule, num_episodes=20000, verbose=True, self_practice=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def heatmaps_subplots(grids, Q):\n",
    "    fig, ax = plt.subplots(1, grids.shape[0], sharex='col', figsize=(15, 5))\n",
    "    plt.subplots_adjust(wspace=1.5)\n",
    "    grids_ = np.reshape(grids, (grids.shape[0], 3, 3))\n",
    "    vmin = np.nanmin([Q[encode_state(grids[i, :])][:] for i in range(grids.shape[0])])\n",
    "    vmax = np.nanmax([Q[encode_state(grids[i, :])][:] for i in range(grids.shape[0])])\n",
    "    for i in range(grids.shape[0]):\n",
    "        grid = grids[i, :]\n",
    "        grid = np.reshape(grid, (3,3))\n",
    "        df = Q[encode_state(grid)][:]\n",
    "        df = np.reshape(df, (3, 3))\n",
    "        ax[i].table(cellText=grids_[i])\n",
    "        sns.heatmap(df, annot=True, mask=(grids_[i] != 0), linewidths=.5, vmin=vmin, vmax=vmax,\n",
    "                    ax=ax[i], xticklabels=False, yticklabels=False, cmap=\"YlGnBu\", \n",
    "                    cbar=False if i < (grids.shape[0]-1) else True, cbar_kws={\"shrink\": 1.2})\n",
    "        ax[i].axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grids = np.array([[1., 0., 0., -1., 1., 0., 0., -1., 0.], [-1., 0., 0., 1., 0., 0., 0., 1., -1.], [1., -1., 0., 0., 0., 0., 0., 0., 0.]])\n",
    "heatmaps_subplots(grids, Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Deep Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS456",
   "language": "python",
   "name": "cs456"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
