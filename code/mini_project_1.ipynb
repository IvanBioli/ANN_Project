{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tic_env import TictactoeEnv, OptimalPlayer\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cmap\n",
    "import seaborn as sns\n",
    "import scipy.stats as st\n",
    "import time\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Configurations\n",
    "save_stats = False\n",
    "save_figs = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def measure_performance(player_1, player_2, num_episodes = 500):\n",
    "    \"\"\"\n",
    "    Measures performance of player 1 against player 2 (percentage of wins player 1)\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    meas = 0\n",
    "    turns = np.array(['X','O'])\n",
    "    env = TictactoeEnv()\n",
    "    for itr in range(num_episodes):\n",
    "        env.reset()\n",
    "        grid, _, __ = env.observe()\n",
    "        player_1.set_player(turns[itr%2])\n",
    "        player_2.set_player(turns[(itr+1)%2])\n",
    "        while not env.end:\n",
    "            if env.current_player == player_1.player:\n",
    "                move = player_1.act(grid)\n",
    "            else:\n",
    "                move = player_2.act(grid)\n",
    "            grid, _, _ = env.step(move, print_grid=False)\n",
    "        meas += env.reward(player=player_1.player)\n",
    "    return meas/num_episodes\n",
    "\n",
    "def running_average(vec, windows_size = 250):\n",
    "    idx = np.arange(0,len(vec),windows_size)\n",
    "    return [np.sum(vec[i:i+windows_size])/windows_size for i in idx], idx + windows_size\n",
    "\n",
    "def encode_state(state):\n",
    "    return state.tobytes()\n",
    "\n",
    "def available(grid):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    avail_indices = []\n",
    "    avail_mask = [False] * 9\n",
    "    for i in range(9):\n",
    "        pos = (int(i/3), i % 3)\n",
    "        if grid[pos] == 0:\n",
    "            avail_indices.append(i)\n",
    "            avail_mask[i] = True\n",
    "    return avail_indices, avail_mask\n",
    "\n",
    "def epsilon_greedy_action(grid, Q, epsilon, num_actions = 9):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    avail_indices, avail_mask = available(grid)\n",
    "\n",
    "    if np.random.uniform(0,1) < epsilon:\n",
    "        return avail_indices[np.random.randint(0, len(avail_indices))]\n",
    "    else:\n",
    "        q = Q[encode_state(grid)]\n",
    "        q[np.logical_not(avail_mask)] = np.nan\n",
    "        max_indices = np.argwhere(q == np.nanmax(q))\n",
    "        return int(max_indices[np.random.randint(0, len(max_indices))])\n",
    "\n",
    "class QPlayer:\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    def __init__(self, Q, player='X'):\n",
    "        self.Q = Q\n",
    "        self.player = player  # 'x' or 'O'\n",
    "\n",
    "    def set_player(self, player='X', j=-1):\n",
    "        self.player = player\n",
    "        if j != -1:\n",
    "            self.player = 'X' if j % 2 == 0 else 'O'\n",
    "\n",
    "    def act(self, grid, **kwargs):\n",
    "        \"\"\"\n",
    "        TODO\n",
    "        \"\"\"\n",
    "        return epsilon_greedy_action(grid, self.Q, 0)\n",
    "\n",
    "\n",
    "def q_learning(env, alpha = 0.5, gamma = 0.99, num_episodes = 20000, epsilon_exploration = 0.1,\n",
    "               epsilon_exploration_rule = None, epsilon_opt = 0.5, test_freq = None, verbose = False):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    turns = np.array(['X','O'])\n",
    "    # Q-values map\n",
    "    # Dictionary that maps the np.ndarray.tobyte() representation of the grid to an array of action values\n",
    "    Q = defaultdict(lambda: np.zeros(9))    # All Q-values are initialized to 0\n",
    "    # Stats of training\n",
    "    episode_rewards = np.zeros(num_episodes)\n",
    "    episode_Mopt = [measure_performance(QPlayer(Q=Q), OptimalPlayer(epsilon=0.))]\n",
    "    episode_Mrand = [measure_performance(QPlayer(Q=Q), OptimalPlayer(epsilon=1.))]\n",
    "    if verbose:\n",
    "                print('Episode  0 :\\tM_opt = ', episode_Mopt[0], '\\tM_rand = ', episode_Mrand[0])\n",
    "    # Rule for exploration\n",
    "    if epsilon_exploration_rule is None:\n",
    "        def epsilon_exploration_rule(n):\n",
    "            return epsilon_exploration\n",
    "\n",
    "    for itr in range(num_episodes):\n",
    "        my_player = turns[itr % 2]\n",
    "        player_opt = OptimalPlayer(epsilon=epsilon_opt, player=turns[(itr+1) % 2])\n",
    "        env.reset()\n",
    "        state, _, _ = env.observe()\n",
    "        # First step of the adversarial\n",
    "        if env.current_player == player_opt.player:\n",
    "            move = player_opt.act(state)\n",
    "            state, _, _ = env.step(move)\n",
    "        action = epsilon_greedy_action(state, Q, epsilon_exploration_rule(itr+1))\n",
    "        while not env.end:\n",
    "            next_state, _, _ = env.step(action)     # Move according to the policy\n",
    "            if not env.end:\n",
    "                move = player_opt.act(next_state)   # Adversarial move\n",
    "                next_state, _, _ = env.step(move)\n",
    "            # Sarsa update rule\n",
    "            reward = env.reward(player=my_player)\n",
    "            if not env.end:\n",
    "                next_action = epsilon_greedy_action(next_state, Q, epsilon_exploration_rule(itr+1))\n",
    "                target = reward + gamma * Q[encode_state(next_state)][next_action]\n",
    "            else:\n",
    "                target = reward\n",
    "            Q[encode_state(state)][action] += alpha * (target - Q[encode_state(state)][action])\n",
    "            # Preparing for the next move\n",
    "            episode_rewards[itr] = reward\n",
    "            state = next_state\n",
    "            action = next_action\n",
    "        # Testing the performance\n",
    "        if (test_freq is not None) and ((itr+1)%test_freq == 0):\n",
    "            M_opt = measure_performance(QPlayer(Q=Q), OptimalPlayer(epsilon=0.))\n",
    "            M_rand = measure_performance(QPlayer(Q=Q), OptimalPlayer(epsilon=1.))\n",
    "            episode_Mopt.append(M_opt)\n",
    "            episode_Mrand.append(M_rand)\n",
    "            if verbose:\n",
    "                print('Episode ', itr+1, ':\\tM_opt = ', M_opt, '\\tM_rand = ', M_rand)\n",
    "    # Dictionary of stats\n",
    "    stats = {\n",
    "        'rewards': episode_rewards,\n",
    "        'test_Mopt': episode_Mopt,\n",
    "        'test_Mrand': episode_Mrand,\n",
    "    }\n",
    "    return Q, stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.1 Learning from experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env = TictactoeEnv()\n",
    "\n",
    "# Hyper-parameters\n",
    "alpha = 0.05    # Learning rate\n",
    "gamma = 0.99    # Discount factor\n",
    "epsilon_opt = 0.5   # Optimal player's epsilon\n",
    "num_episodes = 200\n",
    "epsilon_exploration = 0.01\n",
    "\n",
    "Q, stats = q_learning(env, epsilon_exploration=epsilon_exploration, test_freq = 250, num_episodes=10000, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting the average reward for every 250 games during training\n",
    "running_average_rewards, x = running_average(stats['rewards'])\n",
    "plt.plot(x, running_average_rewards)\n",
    "plt.ylim([-1,1])\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Average reward during training')\n",
    "plt.show()\n",
    "\n",
    "# Comparing the performance with the optimal player and the random player\n",
    "turns = np.array(['X','O'])\n",
    "player_opt = OptimalPlayer(epsilon=0.)\n",
    "player_rand = OptimalPlayer(epsilon=1.)\n",
    "\n",
    "names = ['Optimal', 'Random', 'Trained']\n",
    "players = [OptimalPlayer(epsilon=0.), OptimalPlayer(epsilon=1.), QPlayer(Q=Q)]\n",
    "\n",
    "for (name, player) in zip(names, players):\n",
    "    print(\"\\n-----\", name, \" player-----\")\n",
    "    m_opt = measure_performance(player, player_opt)\n",
    "    print(\"M_opt = \", m_opt)\n",
    "    m_rand = measure_performance(player, player_rand)\n",
    "    print(\"M_rand = \", m_rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 2.1.1 Decreasing exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_freq = 250\n",
    "epsilon_min = 0.1\n",
    "epsilon_max = 0.8\n",
    "vec_n_star = np.hstack((np.array([1, 100, 500, 750]), np.round(np.logspace(3, np.log10(40000), 16))))\n",
    "print(vec_n_star)\n",
    "stats_dict_nstar = {}\n",
    "for n_star in vec_n_star:\n",
    "    print(\"------------- Training with n_star =\", n_star, \"-------------\")\n",
    "    def epsilon_exploration_rule(n):\n",
    "        return np.max([epsilon_min, epsilon_max * (1 - n/n_star)])\n",
    "    start = time.time()\n",
    "    Q, stats = q_learning(env, epsilon_exploration_rule=epsilon_exploration_rule, test_freq=test_freq)\n",
    "    M_opt = measure_performance(QPlayer(Q=Q), OptimalPlayer(epsilon=0.), num_episodes=2000)\n",
    "    M_rand = measure_performance(QPlayer(Q=Q), OptimalPlayer(epsilon=1.), num_episodes=2000)\n",
    "    print(\"M_opt =\", M_opt)\n",
    "    print(\"M_rand =\", M_rand)\n",
    "    stats_dict_nstar.update({n_star: (stats, M_opt, M_rand)})\n",
    "    elapsed = time.time() - start\n",
    "    print(\"Training with n_star =\", n_star, \" took:\", time.strftime(\"%Hh%Mm%Ss\", time.gmtime(elapsed)), \"\\n\\n\")\n",
    "\n",
    "if save_stats:\n",
    "    output_folder = os.path.join(os.getcwd(), 'results')\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    fname = output_folder + '/stats_dict_nstar.pkl'\n",
    "    with open(fname, 'wb') as handle:\n",
    "        pickle.dump(stats_dict_nstar, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_stats(stats_dict, vec_var, var_name, var_legend_name, save = False):\n",
    "    fig_reward, ax_reward = plt.subplots()\n",
    "    fig_performance, ax = plt.subplots(1,2, figsize=(13.4,4.8))\n",
    "\n",
    "    for var in vec_var:\n",
    "        (stats, M_opt, M_rand) = stats_dict[var]\n",
    "        # Plot of the average reward during training\n",
    "        running_average_rewards, x_reward = running_average(stats['rewards'])\n",
    "        ax_reward.plot(x_reward, running_average_rewards, label=\"$\"+ var_legend_name +\" = \"+ str(var) +\"$\")\n",
    "        # Plot of M_opt and M_rand during training\n",
    "        x_performance = np.arange(0, len(stats['rewards'])+1, len(stats['rewards']) / (len(stats['test_Mopt']) - 1))\n",
    "        ax[0].plot(x_performance, stats['test_Mopt'], label=\"$\"+ var_legend_name +\" = \"+ str(var) +\"$\")\n",
    "        ax[1].plot(x_performance, stats['test_Mrand'], label=\"$\"+ var_legend_name +\" = \"+ str(var) +\"$\")\n",
    "        print(var_name + \" =\", var,\": \\tM_opt = \", M_opt, \"\\tM_rand = \", M_rand)\n",
    "\n",
    "    ax_reward.set_ylim([-1,1])\n",
    "    ax_reward.set_xlabel('Episode')\n",
    "    ax_reward.set_ylabel('Reward')\n",
    "    ax_reward.set_title('Average reward during training')\n",
    "    ax_reward.legend(loc='lower right')\n",
    "\n",
    "\n",
    "    ax[0].hlines(y=0, xmin=x[0], xmax=x[-1], color='r', linestyle='--')\n",
    "    ax[0].set_ylim([-1,0.1])\n",
    "    ax[0].set_xlabel('Episode')\n",
    "    ax[0].set_ylabel('$M_{opt}$')\n",
    "    ax[0].set_title('$M_{opt}$ during training')\n",
    "    ax[0].legend(loc='lower right')\n",
    "\n",
    "    ax[1].set_ylim([-1,1])\n",
    "    ax[1].set_xlabel('Episode')\n",
    "    ax[1].set_ylabel('$M_{rand}$')\n",
    "    ax[1].set_title('$M_{rand}$ during training')\n",
    "    ax[1].legend(loc='lower right')\n",
    "    plt.show()\n",
    "    if save:\n",
    "        output_folder = os.path.join(os.getcwd(), 'figures')\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        fig_performance.savefig(output_folder + '/performance_'+var_name+'.png')\n",
    "        fig_reward.savefig(output_folder + '/rewards_'+var_name+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_n_star = vec_n_star\n",
    "plot_stats(stats_dict_nstar, plot_n_star, 'n_star', \"n^{\\star}\", save=save_figs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 2.1.2 Good experts and bad experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_star = 4000 # this should be picked from before not inserted manually\n",
    "#vec_epsilon_opt = [0, 0.3, 0.5, 0.7, 1]\n",
    "M = 10 # how much we want to stratify [0, 1]\n",
    "# a reasonable choice for the epsilon vector here to me is\n",
    "vec_epsilon_opt = np.hstack([np.array([0]), st.uniform.rvs(loc=np.arange(M) / M, scale=1/M), np.array([1])])\n",
    "print(vec_epsilon_opt)\n",
    "stats_dict_epsilon_opt = {}\n",
    "for epsilon_opt in vec_epsilon_opt:\n",
    "    print(\"------------- Training with epsilon_opt =\", epsilon_opt, \"-------------\")\n",
    "    start = time.time()\n",
    "    Q, stats = q_learning(env, epsilon_opt=epsilon_opt, test_freq=test_freq)\n",
    "    M_opt = measure_performance(QPlayer(Q=Q), OptimalPlayer(epsilon=0.), num_episodes=2000)\n",
    "    M_rand = measure_performance(QPlayer(Q=Q), OptimalPlayer(epsilon=1.), num_episodes=2000)\n",
    "    print(\"M_opt =\", M_opt)\n",
    "    print(\"M_rand =\", M_rand)\n",
    "    stats_dict_epsilon_opt.update({epsilon_opt: (stats, M_opt, M_rand)})\n",
    "    elapsed = time.time() - start\n",
    "    print(\"Training with epsilon_opt =\", epsilon_opt, \" took:\", time.strftime(\"%Hh%Mm%Ss\", time.gmtime(elapsed)))\n",
    "\n",
    "if save_stats:\n",
    "    output_folder = os.path.join(os.getcwd(), 'results')\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    fname = output_folder + '/stats_dict_epsilon_opt.pkl'\n",
    "    with open(fname, 'wb') as handle:\n",
    "        pickle.dump(stats_dict_epsilon_opt, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_epsilon_opt = vec_epsilon_opt\n",
    "plot_stats(stats_dict_epsilon_opt, plot_epsilon_opt, \"epsilon_opt\", \"\\epsilon_{opt}\", save=save_figs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.2 Learning by self-practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def q_learning_self_practice(env, alpha = 0.5, gamma = 0.99, num_episodes = 20000,\n",
    "               epsilon_exploration = 0.1, epsilon_exploration_rule = None, epsilon_opt = 0.5,\n",
    "               test_freq = None, verbose = False):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    turns = np.array(['X','O'])\n",
    "    # Q-values map\n",
    "    # Dictionary that maps the np.ndarray.tobyte() representation of the grid to an array of action values\n",
    "    Q = defaultdict(lambda: np.zeros(9))    # All Q-values are initialized to 0\n",
    "    # Stats of training\n",
    "    episode_rewards = np.zeros(num_episodes)\n",
    "    # playing against itself with the same Q-values\n",
    "    episode_Mopt = [measure_performance(QPlayer(Q=Q), OptimalPlayer(epsilon=0.))]\n",
    "    episode_Mrand = [measure_performance(QPlayer(Q=Q), OptimalPlayer(epsilon=1.))]\n",
    "    if verbose:\n",
    "                print('Episode  0 :\\tM_opt = ', episode_Mopt[0], '\\tM_rand = ', episode_Mrand[0])\n",
    "    # Rule for exploration\n",
    "    if epsilon_exploration_rule is None:\n",
    "        def epsilon_exploration_rule(n):\n",
    "            return epsilon_exploration\n",
    "\n",
    "    for itr in range(num_episodes):\n",
    "        my_player = turns[(itr + 1) % 2]\n",
    "        adv_player = turns[itr % 2]\n",
    "        env.reset()\n",
    "        state, _, _ = env.observe()\n",
    "        while not env.end:\n",
    "            if env.current_player == adv_player and not env.end:\n",
    "                action = epsilon_greedy_action(state, Q, epsilon_exploration_rule(itr + 1))\n",
    "                grid, _, _ = env.step(action)\n",
    "                reward =  - env.reward(player=my_player)\n",
    "                if not env.end:\n",
    "                    next_action = epsilon_greedy_action(grid, Q, epsilon_exploration_rule(itr + 1))\n",
    "                    target = reward + gamma * Q[encode_state(grid)][next_action]\n",
    "                else:\n",
    "                    target = reward\n",
    "                Q[encode_state(state)][action] += alpha * (target - Q[encode_state(state)][action])\n",
    "                state = grid\n",
    "            if env.current_player == my_player and not env.end:\n",
    "                action = epsilon_greedy_action(state, Q, epsilon_exploration_rule(itr + 1))\n",
    "                grid, _, _ = env.step(action)\n",
    "                reward = env.reward(player=my_player)\n",
    "                if not env.end:\n",
    "                    next_action = epsilon_greedy_action(grid, Q, epsilon_exploration_rule(itr + 1))\n",
    "                    target = reward + gamma * Q[encode_state(grid)][next_action]\n",
    "                else:\n",
    "                    target = reward\n",
    "                Q[encode_state(state)][action] += alpha * (target - Q[encode_state(state)][action])\n",
    "                state = grid\n",
    "        episode_rewards[itr] = reward\n",
    "        # Testing the performance\n",
    "        if (test_freq is not None) and ((itr+1)%test_freq == 0):\n",
    "            M_opt = measure_performance(QPlayer(Q=Q), OptimalPlayer(epsilon=0.))\n",
    "            M_rand = measure_performance(QPlayer(Q=Q), OptimalPlayer(epsilon=1.))\n",
    "            episode_Mopt.append(M_opt)\n",
    "            episode_Mrand.append(M_rand)\n",
    "            if verbose:\n",
    "                print('Episode ', itr+1, ':\\tM_opt = ', M_opt, '\\tM_rand = ', M_rand)\n",
    "    # Dictionary of stats\n",
    "    \n",
    "    \n",
    "    stats = {\n",
    "        'rewards': episode_rewards,\n",
    "        'test_Mopt': episode_Mopt,\n",
    "        'test_Mrand': episode_Mrand,\n",
    "    }\n",
    "    return Q, stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 7: Fixed $\\epsilon$ first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env = TictactoeEnv()\n",
    "\n",
    "# Hyper-parameters\n",
    "alpha = 0.05    # Learning rate\n",
    "gamma = 0.99    # Discount factor\n",
    "epsilon_opt = 0.5   # Optimal player's epsilon\n",
    "epsilon_vec = [0.01]\n",
    "stats_dic_eps = {}\n",
    "for epsilon in epsilon_vec:\n",
    "    print(\"------------- Training with epsilon =\", epsilon, \"-------------\")\n",
    "    Q, stats = q_learning_self_practice(env, num_episodes=10000,\n",
    "                        epsilon_exploration=epsilon, test_freq = 250, verbose=True)\n",
    "    stats_dic_eps.update({epsilon: stats})\n",
    "grids = np.array([[1, -1, 0, 0, 1, -1, 1, -1, 0], [1, -1, 1, 0, 0, 1, 0, -1, -1], [0, -1, 1, 0, 0, 1, 0, 0, -1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the average reward for every 250 games during training\n",
    "running_average_rewards, x = running_average(stats['rewards'])\n",
    "plt.plot(x, running_average_rewards)\n",
    "plt.ylim([-1,1])\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Average reward during training')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 8: Changing exploration rule, $\\epsilon$ decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_n_star = 10 * np.arange(1, 10)\n",
    "epsilon_min = 0.1\n",
    "epsilon_max = 0.8\n",
    "for n_star in vec_n_star:\n",
    "    print(\"------------- Training with n_star =\", n_star, \"-------------\")\n",
    "    def epsilon_exploration_rule(n):\n",
    "        return np.max([epsilon_min, epsilon_max * (1 - n/n_star)])\n",
    "    start = time.time()\n",
    "    Q, stats = q_learning_self_practice(env, epsilon_exploration_rule=epsilon_exploration_rule,\n",
    "                                        test_freq=test_freq,  num_episodes=2000, verbose=True)\n",
    "    M_opt = measure_performance(QPlayer(Q=Q), OptimalPlayer(epsilon=0.), num_episodes=200)\n",
    "    M_rand = measure_performance(QPlayer(Q=Q), OptimalPlayer(epsilon=1.), num_episodes=200)\n",
    "    print(\"M_opt =\", M_opt)\n",
    "    print(\"M_rand =\", M_rand)\n",
    "    stats_dict_nstar.update({n_star: (stats, M_opt, M_rand)})\n",
    "    elapsed = time.time() - start\n",
    "    print(\"Training with n_star =\", n_star, \" took:\", time.strftime(\"%Hh%Mm%Ss\", time.gmtime(elapsed)), \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 9: TODO look for optimal $M_{opt}$ and $M_{rand}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### look for optimal values of M_opt and M_rand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 10: heat maps in 3 different states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmaps_subplots(grids, Q):\n",
    "    fig, ax = plt.subplots(1, grids.shape[0], sharex='col', figsize=(10, 5))\n",
    "    plt.subplots_adjust(wspace=0.5)\n",
    "    for i in range(grids.shape[0]):\n",
    "        df = Q[encode_state(grids[i, :])][:]\n",
    "        df = np.reshape(df, (3, 3))\n",
    "        sns.heatmap(df, annot=True, linewidths=.5, ax=ax[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grids = np.array([[1, 0, -1, 1, -1, 1, 1, -1, 1], [1, -1, 1, 0, 0, 1, 0, -1, -1], [0, -1, 1, 0, 0, 1, 0, 0, -1]])\n",
    "heatmaps_subplots(grids, Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Deep Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS456",
   "language": "python",
   "name": "cs456"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
